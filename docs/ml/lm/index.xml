<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear modeling on Personal Website</title>
    <link>https://jawad.dev/docs/ml/lm/</link>
    <description>Recent content in Linear modeling on Personal Website</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="https://jawad.dev/docs/ml/lm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction &amp; Background</title>
      <link>https://jawad.dev/docs/ml/lm/1_introduction_and_background/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jawad.dev/docs/ml/lm/1_introduction_and_background/</guid>
      <description>&lt;h2 id=&#34;overview&#34; class=link_owner&gt;&#xA;  Overview&#xA;  &lt;a href=&#34;#overview&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Welcome to &lt;strong&gt;Modeling with Linear &amp;amp; Generalized Linear Models&lt;/strong&gt;!  In this first module we will:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Review the &lt;strong&gt;notation&lt;/strong&gt; and &lt;strong&gt;basic statistical concepts&lt;/strong&gt; that will recur throughout the course.&lt;/li&gt;&#xA;&lt;li&gt;Introduce our running &lt;strong&gt;motivating example&lt;/strong&gt;: predicting house prices using regression.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;By the end of this lesson you should feel comfortable with symbols like \(Y\), \(X\), \(a\), and be ready to formulate and fit your first linear model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simple Linear Regression</title>
      <link>https://jawad.dev/docs/ml/lm/2_simple_linear_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jawad.dev/docs/ml/lm/2_simple_linear_regression/</guid>
      <description>&lt;h2 id=&#34;what-is-linear-regression&#34; class=link_owner&gt;&#xA;  What is Linear Regression?&#xA;  &lt;a href=&#34;#what-is-linear-regression&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Linear regression&lt;/strong&gt; is a prediction model that establishes a linear relationship between a &lt;strong&gt;target variable&lt;/strong&gt; and a set of &lt;strong&gt;explanatory variables&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The case of &lt;strong&gt;one explanatory variable&lt;/strong&gt; is called &lt;strong&gt;simple linear regression&lt;/strong&gt;:&lt;/p&gt;&#xA;\[&#xD;&#xA;\hat{Y} = aX + b&#xD;&#xA;\]&lt;h3 id=&#34;the-gaussmarkov-theorem&#34; class=link_owner&gt;&#xA;  The Gauss–Markov Theorem&#xA;  &lt;a href=&#34;#the-gaussmarkov-theorem&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The &lt;strong&gt;Gauss–Markov theorem&lt;/strong&gt; states that the &lt;strong&gt;ordinary least squares (OLS) estimator&lt;/strong&gt; has the lowest sampling variance within the class of linear unbiased estimators, &lt;strong&gt;if the errors&lt;/strong&gt; in the linear regression model are:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inference &amp; Diagnostic</title>
      <link>https://jawad.dev/docs/ml/lm/3_inference_and_diagnostics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jawad.dev/docs/ml/lm/3_inference_and_diagnostics/</guid>
      <description>&lt;h2 id=&#34;residual-analysis&#34; class=link_owner&gt;&#xA;  Residual Analysis&#xA;  &lt;a href=&#34;#residual-analysis&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Residual analysis is critical for validating our regression assumptions and identifying potential problems.&lt;/p&gt;&#xA;&lt;h3 id=&#34;types-of-residual-plots&#34; class=link_owner&gt;&#xA;  Types of Residual Plots&#xA;  &lt;a href=&#34;#types-of-residual-plots&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-residuals-vs-fitted-values&#34; class=link_owner&gt;&#xA;  1. Residuals vs Fitted Values&#xA;  &lt;a href=&#34;#1-residuals-vs-fitted-values&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;A plot of residuals against fitted values should show &lt;strong&gt;no pattern&lt;/strong&gt;. If a pattern is observed, there may be issues:&lt;/p&gt;&#xA;&#xA;&lt;picture class=&#34;nav_logo&#34; data-lit=&#34;https://jawad.dev/images/residuals_patterns.png&#34; data-dark=&#34;https://jawad.dev/images/Common%20patterns%20in%20residual%20plots&#34;&gt;&#xA;  &lt;img src=&#34;https://jawad.dev/images/residuals_patterns.png&#34; alt=&#34;&#34;&gt;&#xA;&lt;/picture&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;No problem&lt;/strong&gt;: Random scatter around zero&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Heteroscedasticity&lt;/strong&gt;: Variance increases with fitted values&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Nonlinear&lt;/strong&gt;: Curved pattern suggests missing nonlinear terms&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;key-insights-from-residual-plots&#34; class=link_owner&gt;&#xA;  Key Insights from Residual Plots&#xA;  &lt;a href=&#34;#key-insights-from-residual-plots&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Heteroscedasticity&lt;/strong&gt;: The variance of residuals is not constant → violates Gauss–Markov assumptions&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multiple Regression and Feature Engineering</title>
      <link>https://jawad.dev/docs/ml/lm/4_multiple_regression_and_feature_engineering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jawad.dev/docs/ml/lm/4_multiple_regression_and_feature_engineering/</guid>
      <description>&lt;h2 id=&#34;multiple-regression&#34; class=link_owner&gt;&#xA;  Multiple Regression&#xA;  &lt;a href=&#34;#multiple-regression&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;strong&gt;multiple regression model&lt;/strong&gt; relates &lt;strong&gt;more than one predictor&lt;/strong&gt; to a single response variable.&lt;/p&gt;&#xA;&lt;p&gt;For \(p\) predictors plus an intercept, the model is:&lt;/p&gt;&#xA;\[&#xA;Y = a_0 + a_1 X_1 + a_2 X_2 + \ldots + a_p X_p + \varepsilon&#xA;\]&lt;p&gt;where:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;\(Y\) is the target variable&lt;/li&gt;&#xA;&lt;li&gt;\(X_1, X_2, \ldots, X_p\) are the predictors&lt;/li&gt;&#xA;&lt;li&gt;\(a_0, a_1, \ldots, a_p\) are the coefficients&lt;/li&gt;&#xA;&lt;li&gt;\(\varepsilon\) is the error term&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For a single observation, we can write:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model Selection and Regularization</title>
      <link>https://jawad.dev/docs/ml/lm/5_model_selection_and_regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jawad.dev/docs/ml/lm/5_model_selection_and_regularization/</guid>
      <description>&lt;h2 id=&#34;inference-in-regression&#34; class=link_owner&gt;&#xA;  Inference in Regression&#xA;  &lt;a href=&#34;#inference-in-regression&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Under the assumption of &lt;strong&gt;normality of the errors&lt;/strong&gt; \(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\), we can perform statistical inference about the coefficients.&lt;/p&gt;&#xA;&lt;h3 id=&#34;distribution-of-coefficient-estimators&#34; class=link_owner&gt;&#xA;  Distribution of Coefficient Estimators&#xA;  &lt;a href=&#34;#distribution-of-coefficient-estimators&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;From the normal equation \(\hat{a} = (X^T X)^{-1} X^T Y\), we have:&lt;/p&gt;&#xA;\[&#xA;E(\hat{a}) = a&#xA;\]\[&#xA;\text{VAR}(\hat{a}) = \sigma^2 (X^T X)^{-1}&#xA;\]&lt;p&gt;Therefore:&lt;/p&gt;&#xA;\[&#xA;\hat{a} \sim \mathcal{N}(a, \sigma^2 (X^T X)^{-1})&#xA;\]&lt;h3 id=&#34;estimating-the-error-variance&#34; class=link_owner&gt;&#xA;  Estimating the Error Variance&#xA;  &lt;a href=&#34;#estimating-the-error-variance&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The variance of the residuals can be approximated using the &lt;strong&gt;residual sum of squares&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generalized Linear Models (GLM) and Logistic Regression</title>
      <link>https://jawad.dev/docs/ml/lm/6_generalized_linear_models_and_logistic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jawad.dev/docs/ml/lm/6_generalized_linear_models_and_logistic/</guid>
      <description>&lt;h2 id=&#34;generalized-linear-models-glm&#34; class=link_owner&gt;&#xA;  Generalized Linear Models (GLM)&#xA;  &lt;a href=&#34;#generalized-linear-models-glm&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;strong&gt;generalized linear model (GLM)&lt;/strong&gt; is a generalization of ordinary linear regression that allows the response variable to have distributions other than the normal distribution.&lt;/p&gt;&#xA;&lt;h3 id=&#34;the-three-components-of-a-glm&#34; class=link_owner&gt;&#xA;  The Three Components of a GLM&#xA;  &lt;a href=&#34;#the-three-components-of-a-glm&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;A GLM consists of &lt;strong&gt;three elements&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;h4 id=&#34;1-exponential-family-of-probability-distributions&#34; class=link_owner&gt;&#xA;  1. Exponential Family of Probability Distributions&#xA;  &lt;a href=&#34;#1-exponential-family-of-probability-distributions&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;The response variable \(Y\) follows a distribution from the &lt;strong&gt;exponential family&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Annexes</title>
      <link>https://jawad.dev/docs/ml/lm/7_mathematical_annexes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jawad.dev/docs/ml/lm/7_mathematical_annexes/</guid>
      <description>&lt;p&gt;This annex provides detailed mathematical derivations for the key results in linear models.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;annex-a-ols-solution-for-simple-linear-regression&#34; class=link_owner&gt;&#xA;  Annex A: OLS Solution for Simple Linear Regression&#xA;  &lt;a href=&#34;#annex-a-ols-solution-for-simple-linear-regression&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;problem-statement&#34; class=link_owner&gt;&#xA;  Problem Statement&#xA;  &lt;a href=&#34;#problem-statement&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;For simple linear regression:&lt;/p&gt;&#xA;\[&#xD;&#xA;Y_i = a X_i + b + \varepsilon_i&#xD;&#xA;\]&lt;p&gt;We want to minimize the &lt;strong&gt;sum of squared residuals&lt;/strong&gt;:&lt;/p&gt;&#xA;\[&#xD;&#xA;\text{SSE}(a, b) = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} (Y_i - (aX_i + b))^2&#xD;&#xA;\]&lt;h3 id=&#34;derivation&#34; class=link_owner&gt;&#xA;  Derivation&#xA;  &lt;a href=&#34;#derivation&#34; class=&#34;link icon&#34;&gt;&#xA;    &lt;svg&gt;&#xA;      &lt;use xlink:href=&#34;#anchor-link&#34;&gt;&lt;/use&gt;&#xA;    &lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Step 1: Expand the objective function&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
