[{"body":" Objectives By the end, you’ll be able to:\nFit robust predictive models for both regression and classification tasks. Judge feature contributions via confidence intervals, t‑tests, and F‑tests. Select the best subset of predictors for optimal performance. Concepts Covered Simple vs. Multiple Linear Regression Key Assumptions (Gauss–Markov Theorem) Inference \u0026amp; Statistical Testing t‑tests, F‑tests, Confidence Intervals Residual \u0026amp; Diagnostic Analysis Categorical Variables Dummy Encoding, Polynomial Regression Generalized Linear Models Logistic Regression, Link Functions Introduction \u0026amp; Background ","link":"https://jawad.dev/docs/ml/lm/","section":"docs","title":"Linear modeling"},{"body":"This course provides a comprehensive introduction to Machine Learning, covering both theoretical foundations and practical applications. It introduces students to supervised and unsupervised learning, focusing on model evaluation, feature selection, clustering, decision trees, and regression techniques.\nStudents will develop hands-on expertise in building, tuning, and evaluating models while understanding the mathematical principles behind them. The course emphasizes best practices in data preprocessing, model selection, and performance optimization, equipping students with the necessary tools to apply Machine Learning in real-world scenarios across various industries.\nLinear \u0026amp; Logistic Regression Simple vs. Multiple Linear Regression Assumptions (Gauss-Markov Theorem) Model Inference \u0026amp; Statistical Testing t-tests F-tests Confidence Intervals Handling Categorical Variables Dummy Encoding Polynomial Regression Logistic Regression for Classification Practical Example: Predicting house prices with multiple regression Decision Trees Tree-Based Model Structure Splitting Criteria Gini Index Entropy Overfitting Prevention Pre-Pruning Post-Pruning Computational Complexity Considerations Practical Example: Decision tree classification on real-world data Model Evaluation \u0026amp; Feature Selection Understanding Bias-Variance Trade-off Cross-Validation Techniques Evaluation Metrics MAE RMSE R² AUC-ROC Confusion Matrix Feature Selection Techniques Wrapper Filter Regularization Hyperparameter Tuning Grid Search Bayesian Optimization AutoML Practical Example: Model selection and tuning using Python Clustering Techniques Introduction to Unsupervised Learning Hierarchical Clustering Agglomerative Divisive Partitioning Methods K-Means K-Medoids Density-Based Clustering DBSCAN OPTICS Gaussian Mixture Models Expectation-Maximization Practical Example: Clustering customer data for segmentation Get started now ","link":"https://jawad.dev/docs/ml/","section":"docs","title":"Maching Learning"},{"body":"These documentation pages are organised into three self‑contained modules:\nMaching Learning (ongoing) – fundamentals of supervised / unsupervised learning with hands‑on model building. Deep Learning (ongoing) – neural‑network theory, modern architectures (CNNs, RNNs, Transformers) and practical implementation. Explainability in AI (ongoing) – methods for interpreting models and ensuring trust, fairness, and regulatory compliance. Each module can be followed independently, but new learners are encouraged to begin with Maching Learning, then continue to Deep Learning and Explainability.\nMaching Learning ","link":"https://jawad.dev/docs/","section":"docs","title":"Teaching Docs"},{"body":"Do you prefer managing your site using a CMS? Or would you like to make it easier for someone (a non-techie, perhaps) in your team to make edits easily? If interested, follow along. Else, skip to the next section\nLet's sync your site with Tina CMS.\nPrerequisites !! Obviously you ought to have a github account. This is where your website source will live. Basically, Tina will read from github and write (commit) to your github repo.\nGitlab or bitbucket will work too. Just check their implementation here. Happy fishing. Requirement 1 : A Tina.io account Jump over to Tina.io and sign up for an account. Consider signing up using your github account. That way, you don't have to deal with passwords.\nRequirement 2: A Netlify account (optional) If you intend to host with something other than Netlify e.g github pages, please scroll on. Hosting with Netlify is a lot of fun though; I highly recommend it.\nStep 1 : Fork or Clone Compose theme First we will fork this theme's template.\nStep 2 : Add your repository in Tina CMS The exampleSite already comes with prefilled placeholder Tina settings. If you set up your site using option 2 Edit ./static/tina/config.js and replace tina CMS tokens with values from your own Tina account\n1... 2 clientId: \u0026#34;6ff9830b-b18a-4d21-b38c-cae1679e335f\u0026#34;, // replace 3 token: \u0026#34;2954980a0db18868974dc57a66be9ecdfe6f336b\u0026#34;, // replace 4... 5search: { 6 ... 7 tina: { 8 indexerToken: \u0026#34;977c145439dda036080dd7a33478d2ba385ab5af\u0026#34;, // replace 9 stopwordLanguages: [\u0026#34;deu\u0026#34;, \u0026#34;eng\u0026#34;, \u0026#34;fra\u0026#34;, \u0026#34;ita\u0026#34;, \u0026#34;spa\u0026#34;, \u0026#34;nld\u0026#34;] // consider adding or removing languages https://github.com/fergiemcdowall/stopword#language-code 10 }, 11 ... 12 } 13... Go to your Tina account\n","link":"https://jawad.dev/docs/dl/use-tina-cms/","section":"docs","title":"Use Tina.io CMS"},{"body":"This theme is primarily meant for documentation.\nDocumentation By default, the theme will look for all your documentation content within the docs directory.\nHowever, if you would like to have your docs content across multiple directories, please list those directories inside config/_default/params.toml under docSections like so:\n...\rdocSections = [\u0026#34;docs\u0026#34;, \u0026#34;tutorials\u0026#34;]\r... Unlike other regular pages, the documentation pages will have a left sidebar. This sidebar will list links to all the pages in the documentation pages. Beneath each link, there will be a collapsible list of table of contents' links. These nested lists will unfold automatically on the active/current page.\nHome Page At the root level there's an _index.md page which is the homepage. Feel free to edit it as you like.\nOther pages You can also add as many regular pages as you like e.g about.md, contact.md...\nTake advantage of shortcodes to customize the layouts of these pages and any other.\nDoes this theme support blogging function? Currently, no.\n","link":"https://jawad.dev/docs/xai/organize-content/","section":"docs","title":"Content organization"},{"body":"Instead of writing all your site pages from scratch, Hugo lets you define and use shortcodes.\nWhy shortcodes? While markdown is sufficient to produce simple pages, it's insufficient where complex page structures are needed. Thusly, whenever we need special styling, shortcodes compliment the shortcomings of markdown.\nThis way, you can side step complex html and css boilerplate in your content files.\nSometimes, the shortcode will wrap content, sometimes it won't. When content is wrapped, a closing shortcode tag is needed. Please see the link I provided above and the markdown files for examples. You'll get the gist pretty quickly.\nI've setup the following shortcodes:\nBlock Takes positional modifiers\nExample\n1... 2 {{\u0026lt; block \u0026#34;modifiers\u0026#34; \u0026gt;}} 3 \u0026lt;!-- Nest columns or content --\u0026gt; 4 {{\u0026lt; /block \u0026gt;}} 5... Column It takes positional parameters\nExample\n1 {{\u0026lt; column \u0026#34;mt-2 mb-2\u0026#34; \u0026gt;}} 2 \u0026lt;!-- applied margin top and margin bottom modifiers --\u0026gt; 3 {{\u0026lt; /column \u0026gt;}} Youtube Video This allows you to embed a youtube video in you content. You would achieve that using a positional parameter (needs no name )parameter, like so:\nSyntax\n1 {{\u0026lt; youtube \u0026#34;25QyCxVkXwQ\u0026#34; \u0026gt;}} 2 \u0026lt;!-- Use the youtube video id --\u0026gt; Result\nOR\nSyntax\n1\u0026lt;!-- or use full url --\u0026gt; 2{{\u0026lt; youtube \u0026#34;https://www.youtube.com/watch?v=MmG2ah5Df4g\u0026#34; \u0026gt;}} Result\nLite YouTube The liteyoutube shortcode supports three parameters:\nPARAMETER PURPOSE OPTIONAL videoid YouTube video identifier no params YouTube parameters yes img Background image from static/images yes With no Parameters This example shows only supplying the required videoid (without a named parameter). You can also add the img and params parameters (in that order) without using named parameters.\n1{{\u0026lt; liteyoutube \u0026#34;MmG2ah5Df4g\u0026#34; \u0026gt;}} With videoid and params The params string instructs YouTube to play only 20 seconds of the video starting at ten seconds and ending at 30 seconds. It also disables the player controls and enables the YouTube JSAPI.\n1{{\u0026lt; liteyoutube videoid=\u0026#34;MmG2ah5Df4g\u0026#34; params=\u0026#34;controls=0\u0026amp;start=10\u0026amp;end=30\u0026amp;modestbranding=2\u0026amp;rel=0\u0026amp;enablejsapi=1\u0026#34; \u0026gt;}} With All Three Positional Parameters 1{{\u0026lt; liteyoutube \u0026#34;MmG2ah5Df4g\u0026#34; \u0026#34;painting.jpg\u0026#34; \u0026#34;controls=0\u0026amp;start=10\u0026amp;end=30\u0026amp;modestbranding=2\u0026amp;rel=0\u0026amp;enablejsapi=1\u0026#34; \u0026gt;}} You can browse the full list of YouTube parameters here Button This adds a styled link (styled like a button). It takes two no-optional parameters:\nPARAMETER PURPOSE OPTIONAL label button text no url button link no modifier styling classes yes Example\n1 {{\u0026lt; button \u0026#34;/\u0026#34; \u0026#34;doe nu mee\u0026#34; \u0026gt;}} Picture You want to use darkmode images when darkmode is enabled on a device and a regular image on lightmode? It takes 3 positional parameter\nStore these images in the static/images directory.\nSyntax\n1... 2{{\u0026lt; picture \u0026#34;lightModeImage.png\u0026#34; \u0026#34;darkModeImage.png\u0026#34; \u0026#34;Image alt text\u0026#34; \u0026gt;}} 3... Result\nGallery Include inline galleries within your articles. These galleries can contain N number of images. It takes 2 positional parameters.\nThe 1st parameter is required. It's a comma-separated list (,) of your images' paths.\nThe 2nd parameter is optional. It's a double-collon-separated list (::) of your images' alt/description/captions text. It's always a good SEO practice to include alt text for your images.\nSyntax\n1... 2{{\u0026lt; gallery \u0026#34;images/painting.jpg,images/scribble.jpg,images/painting.jpg\u0026#34; \u0026#34;Gallery Image 1::gallery image 2::gallery image 1 copy\u0026#34; \u0026gt;}} 3... For legibility, you may include a space after the delimiters , \u0026amp; ::\nResult\nTab(s) Use this short if you want to publish a multiple tabs component.\nSyntax\n1{{\u0026lt; tabs \u0026#34;tabsId\u0026#34; \u0026gt;}} 2{{\u0026lt; tab \u0026#34;First\u0026#34; \u0026gt;}} 3What could such a tab include? 4{{\u0026lt; /tab \u0026gt;}} 5{{\u0026lt;/* tab \u0026#34;Second\u0026#34; \u0026gt;}} 6- Some *bulletpoints* 7- and more… 8- … 9{{\u0026lt; /tab \u0026gt;}} 10{{\u0026lt;/* tab \u0026#34;Third\u0026#34; \u0026gt;}} 11\u0026gt; great wise words? 12{{\u0026lt; /tab \u0026gt;}} 13{{\u0026lt; /tabs \u0026gt;}} Result\nFirst What could such a tab include? Second Some bulletpoints and more… … Third great wise words?\nTip Use this short if you want to publish informational tooltips that look like:\nThis tooltips may take either of the following forms:\nSyntax\n1{{\u0026lt; tip \u0026gt;}} 2Something of __interest__ you want to highlight 3{{\u0026lt; /tip \u0026gt;}} Result\nSomething of interest you want to highlight OR\nSyntax\n1{{\u0026lt; tip \u0026#34;warning\u0026#34; \u0026gt;}} 2Something of __interest__ the user should be careful about 3{{\u0026lt; /tip \u0026gt;}} Result\nSomething of interest the user should be careful about ","link":"https://jawad.dev/docs/dl/shortcodes/","section":"docs","title":"Shortcodes"},{"body":"Firstly, ensure you have these lines inside your hugo.toml file\n1[outputs] 2 home = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;,\u0026#34;JSON\u0026#34;] Compose implements Fuse js or Algolia to enable search functionality. By default Fuse is applied. Algolia can be enabled by adding this settings to config/_default/params.toml file\n1# search 2[search] 3on = true 4global = false 5[search.algolia] 6enable = false # if false search will default to fusejs 7id = \u0026#34;Q40WQQX84U\u0026#34; # Application ID 8index = \u0026#34;compose\u0026#34; # Index name 9key = \u0026#34;da87401a458102ec6bbd6cc5e5cf8d95\u0026#34; # Search-Only API Key Both search engines will display results using the same UI. By choosing the default (.ie fuse js), you will be opting for local search. This way, no additional setup is needed.\nAlgolia will require you to build and host your index. For those using Github, this theme ships with an algolia github action.\nBy default, search will return results from the current content section. Searches from the top level section e.g the homepage, will return global results. This way, the results are scoped. You can override this behaviour using this setting\n1... 2[search] 3... 4global = false # turn to `true` to enable global search 5... At the time of this writing, search on these theme takes either of this forms:\n1. Passive search This occurs only when the user loads the search page i.e /search/. They can directly navigate to that url. Alternatively, the user can type you search query on the search field and click enter. They will be redirected to the search page which will contain matched results if any.\n2. Live search This behaviour will be obvious as the user types a search query on the search field. All valid search queries, will yield a list of quick links or a simple no matches found. Else, the user will be prompted to continue typing.\nPlease note that the results under quick links will be a truncated list of the most relevant results. Only a maximum of 8 items will be returned. This number is pragmatic at best if not arbitrary. On the search page, the number is set to 12.\nNote that live search on the search page will behave differently than on the other pages. Nonetheles, the pages apply the same live search principle.\nHitting enter while typing on the search page will be moot as that page’s content will live update as you type in the search word / phrase.\nCustomize search feedback labels Use the i18n files to do so.\nWhat is a valid search query A valid search query must be long enough. If the search query can be cast as a float, then it only need contain one or more characters.\nElse the search query must be at least 2 characters long.\n","link":"https://jawad.dev/docs/xai/search/","section":"docs","title":"Search Function"},{"body":" Shortcodes modifiers These modifiers are classes you can use with shortcodes to customize the look and feel of your layouts and components.\nGrid modifier space grid-2 2 columns grid-3 3 columns grid-4 4 columns Spacing modifier space mt-1 1.5rem top margin mt-2 3rem top margin mt-3 4.5rem top margin mt-4 6rem top margin use pt-1 ~ pt-4 for top padding\nmodifier space mb-1 1.5rem bottom margin mb-2 3rem bottom margin mb-3 4.5rem bottom margin mb-4 6rem bottom margin use pb-1 ~ pb-4 for bottom padding\nHow do I disable dark mode? Under params add enableDarkMode = false to your hugo.toml file. If your site is based on the exampleSite, the value is already included; you only need to uncomment it.\nThe user will still have the option to activate dark mode, if they so wish through the UI\nHow do I change the theme color? If the theme is a git submodule, you can copy the file assets/sass/_variables.sass from the theme into your own site. The location must be exactly the same as in the theme, so put it in YourFancySite/assets/sass/. You can then edit the file to customize the theme color in your site without having to modify the theme itself.\nHow can I change the address bar color on mobile devices? Just put the following line in the [params] section in your hugo.toml file (and of course change the color):\n1metaThemeColor = \u0026#34;#123456\u0026#34; How do I add custom styles, scripts, meta tags e.t.c Use hooks. Ideally, you should not override the them directly.\nInstead, you should duplicate these files at the root of you site directory.\nlayouts/partials/hooks/head.html layouts/partials/hooks/scripts.html The contents of the first file will be attached just before the \u0026lt;/head\u0026gt; tag.\nThe contents of the second file will be attached just before the \u0026lt;/body\u0026gt; tag.\nAlternatively, if you want to use the hugo.toml to track your custom styles or scripts, declare them as slices under [params] like so:\n1... 2[params] 3customCSS = [styleURL1, styleURL2 ...] 4customJS = [scriptURL1, scriptURL2 ... ] 5... I want to add custom SASS or JS Add custom SASS and JS via this custom SASS file and this custom JavaScript file.\nHow to change site favicon Your favicons should be stored inside static/favicons directory.\nHere are some of the favicon files that you should have in that folder:\n.\r├── android-chrome-192x192.png\r├── android-chrome-512x512.png\r├── apple-touch-icon.png\r├── favicon-16x16.png\r├── favicon-32x32.png\r├── favicon.ico\r└── site.webmanifest We recommend you consider using this tool while generating your favicons.\n","link":"https://jawad.dev/docs/xai/customize/","section":"docs","title":"Customize layouts \u0026 components"},{"body":" Overview Welcome to Modeling with Linear \u0026amp; Generalized Linear Models! In this first module we will:\nReview the notation and basic statistical concepts that will recur throughout the course. Introduce our running motivating example: predicting house prices using regression. By the end of this lesson you should feel comfortable with symbols like \\(Y\\), \\(X\\), \\(a\\), and be ready to formulate and fit your first linear model.\nNotation Throughout the course we will use the following conventions:\nSymbol Meaning \\(n\\) Number of observations (data points) \\(p\\) Number of predictors (features) including intercept \\(X_i\\) or \\(x_i\\) Features (predictors) for observation \\(i\\) \\(Y_i\\) or \\(y_i\\) The response (target) for observation \\(i\\) \\(X\\) The \\(n\\times p\\) design matrix \\(a\\) or \\(a_j\\) Model coefficients (slope, intercept) \\(\\varepsilon_i\\) Random error term for observation \\(i\\) \\(\\bar{X}\\), \\(\\bar{Y}\\) Sample means for random variables \\(s^2\\) Sample variance Before fitting models, let’s recall some foundational ideas:\nRandom Variables \u0026amp; Expectations A random variable \\(Y\\) has a probability distribution. The expected value (mean) is\n\\[\r\\mathbb{E}[Y] = \\mu = \\int y\\,dF_Y(y)\r\\] The variance measures spread:\n\\[\r\\mathrm{Var}(Y) = \\mathbb{E}\\bigl[(Y - \\mu)^2\\bigr] = \\sigma^2.\r\\] Sampling \u0026amp; Estimation The sample mean of \\(y_1,\\dots,y_n\\): \\[\r\\bar y = \\frac1n\\sum_{i=1}^n y_i.\r\\] The sample variance: \\[\rs^2 = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar y)^2.\r\\] Covariance \u0026amp; Correlation Covariance of two random variables \\(X\\) and \\(Y\\): \\[\r\\mathrm{Cov}(X,Y) = \\mathbb{E}\\bigl[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\bigr].\r\\] Correlation (Pearson’s \\(\\rho\\)) is the standardized covariance: \\[\r\\rho_{XY} = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\,\\mathrm{Var}(Y)}}.\r\\] We will see how these quantities enter into formulas for parameter estimates, tests, and confidence intervals.\nMotivating Example: House Price Prediction To ground our discussion, we’ll work throughout the course with a real-world dataset of house sales. Our goal:\nPredict the sale price \\(y\\) of a home from features such as:\nSize (\\(\\text{sqft}\\)) Number of bedrooms Age of the home Neighborhood quality (that will be encoded as dummy variables) In notation: \\[\ry_i \\;=\\; a_0 + a_1\\,\\text{sqft}_i + a_2\\,\\text{beds}_i + a_3\\,\\text{age}_i + \\cdots + \\varepsilon_i.\r\\]Here is a sample of 10 lines:\nSize Number of bedrooms Age Neighborhood Price 1710 3 5 CollgCr 208500 1262 3 31 Veenker 181500 1786 3 7 CollgCr 223500 1717 3 91 Crawfor 140000 2198 4 8 NoRidge 250000 1362 1 16 Mitchel 143000 1694 3 3 Somerst 307000 2090 3 36 NWAmes 200000 1774 2 77 OldTown 129900 1077 2 69 BrkSide 118000 Here’s your scatter plot of Price (y) vs. Size (x): Later modules will show how to:\nEstimate the coefficients \\(a\\) (Ordinary Least Squares) Test hypotheses about feature effects (t-tests, F-tests) Check model assumptions (residual diagnostics) Extend to generalized linear models (e.g., logistic regression) With our notation and basic statistics clear, we’re ready to dive in to Simple Linear Regression in the next lesson!\n","link":"https://jawad.dev/docs/ml/lm/1_introduction_and_background/","section":"docs","title":"Introduction \u0026 Background"},{"body":" What is Linear Regression? Linear regression is a prediction model that establishes a linear relationship between a target variable and a set of explanatory variables.\nThe case of one explanatory variable is called simple linear regression:\n\\[\r\\hat{Y} = aX + b\r\\] The Gauss–Markov Theorem The Gauss–Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are:\nUncorrelated Have equal variances (homoscedasticity) Have expectation value of zero The full model with error term:\n\\[\rY = aX + b + \\varepsilon\r\\]where \\(\\varepsilon\\) represents the random error.\nOrdinary Least Squares (OLS) Objective Function We need to minimize the sum of squared residuals:\n\\[\r\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} (Y_i - (aX_i + b))^2\r\\] Solution By setting the partial derivatives to zero, we find:\n\\[\r\\hat{a} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}\r\\]\\[\r\\hat{b} = \\bar{Y} - \\hat{a}\\,\\bar{X}\r\\]where:\n\\(\\bar{X}\\) and \\(\\bar{Y}\\) are the sample means \\(\\text{Cov}(X,Y)\\) is the covariance between \\(X\\) and \\(Y\\) \\(\\text{Var}(X)\\) is the variance of \\(X\\) Covariance and Correlation Empirical Covariance The empirical covariance is defined as:\n\\[\r\\text{Cov}(X,Y) = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})\r\\]Covariance signifies the direction of the linear relationship between two variables:\nPositive covariance: variables tend to move in the same direction Negative covariance: variables tend to move in opposite directions Note: \\(\\text{Cov}(X,X) = \\text{Var}(X)\\) Correlation Correlation refers to the scaled form of covariance:\n\\[\r\\text{Cor}(X,Y) = \\frac{\\text{Cov}(X,Y)}{s_X \\cdot s_Y}\r\\]where \\(s_X\\) and \\(s_Y\\) are the standard deviations of \\(X\\) and \\(Y\\).\nProperties:\nCorrelation is dimensionless and ranges from \\(-1\\) to \\(+1\\) \\(|\\text{Cor}(X,Y)| = 1\\) indicates a perfect linear relationship \\(\\text{Cor}(X,Y) = 0\\) indicates no linear relationship Errors and Residuals Statistical Error (Disturbance) A statistical error \\(\\varepsilon_i\\) is the amount by which an observation differs from its expected value:\n\\[\rY_i = aX_i + b + \\varepsilon_i \\quad \\Rightarrow \\quad \\varepsilon_i = Y_i - (aX_i + b)\r\\] Residual A residual \\(r_i\\) is the observable estimate of the unobservable statistical error:\n\\[\rr_i = \\hat{\\varepsilon}_i = Y_i - (\\hat{a}X_i + \\hat{b}) = Y_i - \\hat{Y}_i\r\\]Key difference:\nErrors \\(\\varepsilon_i\\) involve the true (unknown) parameters \\(a\\) and \\(b\\) Residuals \\(r_i\\) involve the estimated parameters \\(\\hat{a}\\) and \\(\\hat{b}\\) Visualizing OLS Fit The regression line minimizes the sum of squared vertical distances (residuals) from each point to the line.\nSummary In this lesson we covered:\n✅ The definition of simple linear regression ✅ The Gauss–Markov theorem and OLS properties ✅ How to derive coefficient estimates using calculus ✅ The relationship between covariance and correlation ✅ The distinction between errors and residuals Next: We'll explore how to assess the quality of our fit, test hypotheses about coefficients, and diagnose potential problems through residual analysis.\n","link":"https://jawad.dev/docs/ml/lm/2_simple_linear_regression/","section":"docs","title":"Simple Linear Regression"},{"body":" Residual Analysis Residual analysis is critical for validating our regression assumptions and identifying potential problems.\nTypes of Residual Plots 1. Residuals vs Fitted Values A plot of residuals against fitted values should show no pattern. If a pattern is observed, there may be issues:\nNo problem: Random scatter around zero Heteroscedasticity: Variance increases with fitted values Nonlinear: Curved pattern suggests missing nonlinear terms Key Insights from Residual Plots Heteroscedasticity: The variance of residuals is not constant → violates Gauss–Markov assumptions\nSolution: Transform the target variable (logarithm, square root) Nonlinearity: Scatterplots of residuals vs predictors show patterns\nSolution: Add polynomial terms or other transformations Studentized Residuals To identify outliers, we standardize residuals. Under the assumption of normality of errors:\n\\[ t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}} \\]where:\n\\(\\hat{\\sigma}\\) is the estimated standard deviation of residuals: \\[ \\hat{\\sigma} = \\sqrt{\\frac{1}{n-2}\\sum_{j=1}^{n}\\hat{\\varepsilon}_j^2} \\] \\(h_{ii}\\) is the leverage (diagonal element of the hat matrix): \\[ h_{ii} = \\frac{1}{n} + \\frac{(X_i - \\bar{X})^2}{\\sum_{j=1}^{n}(X_j - \\bar{X})^2} \\] Under the null hypothesis, \\(t_i\\) follows a Student's t-distribution with \\(n-2\\) degrees of freedom.\nOutlier Detection Studentized residuals help identify observations that don't fit the model:\nPoints with \\(|t_i| \u003e 2\\) or \\(|t_i| \u003e 3\\) are potential outliers Target Transformation When to transform? Target transformation is necessary to cope with heteroscedasticity, which is often linked to the skewness of the target distribution.\nCommon Transformations Problem Transformation Effect Right-skewed target \\(\\log(Y)\\) Reduces skewness, stabilizes variance Variance increases with Y \\(\\sqrt{Y}\\) Moderate stabilization Count data \\(\\log(Y+1)\\) Handles zeros Example: Housing prices are often right-skewed. After log transformation, the distribution becomes more symmetric and residual variance is more constant.\nDiagnostic Plots Modern statistical software provides multiple diagnostic plots. Here's what each reveals:\n1. Residuals vs Fitted Purpose: Detect non-linearity and heteroscedasticity Ideal: Random scatter with no pattern\n2. Normal Q-Q Plot Purpose: Check normality of residuals Ideal: Points lie on the diagonal line\nIf residuals deviate from the line:\nHeavy tails: Points curve away at extremes Skewness: Systematic deviation on one side 3. Scale-Location Plot Purpose: Check homoscedasticity (equal variance) Ideal: Horizontal line with evenly spread points\n4. Residuals vs Leverage Purpose: Identify influential observations Uses: Cook's distance\nInfluential Observations Cook's Distance Cook's distance \\(D_i\\) measures how much the regression coefficients change when observation \\(i\\) is removed:\n\\[ D_i = \\frac{\\sum_{j=1}^{n}(\\hat{y}_j - \\hat{y}_{j(i)})^2}{p\\hat{\\sigma}^2} \\]where \\(\\hat{y}_{j(i)}\\) is the fitted value when observation \\(i\\) is excluded.\nRule of thumb:\n\\(D_i \u003e 0.5\\): Potentially influential \\(D_i \u003e 1\\): Highly influential DFFITS DFFITS (Difference in Fits) is a scaled measure of change in the predicted value:\n\\[ \\text{DFFITS}_i = t_i \\sqrt{\\frac{h_{ii}}{1 - h_{ii}}} \\]Cutoff (Belsley, Kuh, and Welsch): \\[ 2\\sqrt{\\frac{p}{n}} \\] Coefficient of Determination (R²) \\(R^2\\) measures the proportion of variance in \\(Y\\) explained by \\(X\\):\n\\[ R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{\\sum(Y_i - \\hat{Y}_i)^2}{\\sum(Y_i - \\bar{Y})^2} \\]where:\nSST (Total Sum of Squares) = \\(\\sum(Y_i - \\bar{Y})^2\\) SSR (Regression Sum of Squares) = \\(\\sum(\\hat{Y}_i - \\bar{Y})^2\\) SSE (Error Sum of Squares) = \\(\\sum(Y_i - \\hat{Y}_i)^2\\) Interpretation:\n\\(R^2 = 0\\): Model explains no variance (no better than the mean) \\(R^2 = 1\\): Perfect fit (all variance explained) For simple linear regression: \\(R^2 = \\text{Cor}(Y, \\hat{Y})^2\\) Summary In this lesson we covered:\n✅ Residual analysis to detect violations of assumptions ✅ Studentized residuals for outlier detection ✅ Target transformations to handle heteroscedasticity ✅ Diagnostic plots (Q-Q, scale-location, leverage) ✅ Influential observations (Cook's distance, DFFITS) ✅ R² as a measure of model fit Next: We'll extend to multiple regression with several predictors, learning matrix notation and the normal equation.\n","link":"https://jawad.dev/docs/ml/lm/3_inference_and_diagnostics/","section":"docs","title":"Inference \u0026 Diagnostic"},{"body":" Multiple Regression The multiple regression model relates more than one predictor to a single response variable.\nFor \\(p\\) predictors plus an intercept, the model is:\n\\[ Y = a_0 + a_1 X_1 + a_2 X_2 + \\ldots + a_p X_p + \\varepsilon \\]where:\n\\(Y\\) is the target variable \\(X_1, X_2, \\ldots, X_p\\) are the predictors \\(a_0, a_1, \\ldots, a_p\\) are the coefficients \\(\\varepsilon\\) is the error term For a single observation, we can write:\n\\[ y = (x_1, x_2, \\ldots, x_p) \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{pmatrix} + \\varepsilon \\] Matrix Notation For a sample of \\(n\\) observations, we use matrix notation to represent the model compactly.\nThe Design Matrix The full model can be written as:\n\\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \u0026= \\begin{pmatrix} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1p} \\\\ x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2p} \\\\ x_{31} \u0026 x_{32} \u0026 \\cdots \u0026 x_{3p} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ x_{n1} \u0026 x_{n2} \u0026 \\cdots \u0026 x_{np} \\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\varepsilon_3 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\end{aligned} \\]Where:\nResponse vector \\(Y\\): \\(n \\times 1\\) matrix containing all observed responses Design matrix \\(X\\): \\(n \\times p\\) matrix of predictor values Coefficient vector \\(a\\): \\(p \\times 1\\) matrix of parameters to estimate Error vector \\(E\\): \\(n \\times 1\\) matrix of random errors In compact form:\n\\[ Y = Xa + E \\]For regression with intercept, we set the first element of each row to 1:\n\\[ x_{i1} = 1 \\quad \\forall i \\]This allows the intercept \\(a_0\\) to be included as the first coefficient.\nMatrix Operations Review Matrix Multiplication The product of an \\(m \\times n\\) matrix \\(A\\) and an \\(n \\times p\\) matrix \\(B\\) is an \\(m \\times p\\) matrix \\(C\\):\n\\[ C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj} \\] Example:\n\\[ \\begin{aligned} \\begin{pmatrix} 1 \u0026 2 \u0026 3 \\\\ 4 \u0026 5 \u0026 6 \\end{pmatrix} \\begin{pmatrix} 7 \\\\ 8 \\\\ 9 \\end{pmatrix} \u0026= \\begin{pmatrix} (1)(7)+(2)(8)+(3)(9) \\\\ (4)(7)+(5)(8)+(6)(9) \\end{pmatrix} \\\\ \u0026= \\begin{pmatrix} 50 \\\\ 122 \\end{pmatrix} \\end{aligned} \\] Matrix Transpose The transpose \\(A^T\\) of a matrix \\(A\\) is obtained by flipping rows and columns:\n\\[ (A^T)_{ij} = A_{ji} \\]Example:\n\\[ \\begin{aligned} \\begin{pmatrix} 1 \u0026 2 \u0026 3 \u0026 4 \\\\ 5 \u0026 6 \u0026 7 \u0026 8 \\\\ 9 \u0026 10 \u0026 11 \u0026 12 \\end{pmatrix}^{\\mathsf T} \u0026= \\begin{pmatrix} 1 \u0026 5 \u0026 9 \\\\ 2 \u0026 6 \u0026 10 \\\\ 3 \u0026 7 \u0026 11 \\\\ 4 \u0026 8 \u0026 12 \\end{pmatrix} \\end{aligned} \\] Matrix Inverse A matrix \\(B\\) is the inverse of \\(A\\) if:\n\\[ B \\cdot A = A \\cdot B = I_n \\]where \\(I_n\\) is the \\(n \\times n\\) identity matrix.\nNotation: \\(B = A^{-1}\\)\nImportant properties:\nOnly square matrices can have an inverse A matrix that is not invertible is called singular or degenerate A matrix is invertible if it has full column rank \\(n\\) (no perfect multicollinearity) Application to solving systems:\n\\[ AX = Y \\quad \\Leftrightarrow \\quad X = A^{-1}Y \\] Norm of a Vector The Euclidean norm (or \\(\\ell_2\\)-norm) of a vector \\(v\\) is:\n\\[ \\|v\\|^2 = v^T v = v_1^2 + v_2^2 + \\ldots + v_p^2 \\]Example:\nFor \\(v = \\begin{pmatrix} -1 \\\\ -2 \\\\ 3 \\\\ 4 \\end{pmatrix}\\):\n\\[ \\|v\\| = \\sqrt{(-1)^2 + (-2)^2 + 3^2 + 4^2} = \\sqrt{1 + 4 + 9 + 16} = \\sqrt{30} \\] Ordinary Least Squares (OLS) in Matrix Form We need to find the vector \\(a\\) that minimizes:\n\\[ \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\left( y_i - \\sum_{j=1}^{p} a_j x_{ij} \\right)^2 = \\|Y - X \\cdot a\\|^2 \\] Normal Equation The analytical solution for OLS is given by the normal equation:\n\\[ \\hat{a} = (X^T X)^{-1} X^T Y \\]Assumptions:\n\\(X\\) has full column rank: \\(\\text{rank}(X) = p\\) This ensures \\(X^T X\\) is invertible Equivalently: \\(\\text{rank}(X^T X) = \\text{rank}(XX^T) = \\text{rank}(X)\\) When Normal Equation Fails The normal equation cannot be used if:\nPerfect multicollinearity: One predictor is a linear combination of others\nExample: \\(X_3 = 2X_1 + 3X_2\\) Result: Parameters are non-identifiable (no unique solution) Too few observations: \\(n \u003c p\\)\nFewer data points than parameters to estimate \\(\\text{rank}(X) \\leq \\min(n, p)\\) Note: If predictors are highly but not perfectly correlated, \\(X^T X\\) is technically invertible, but the inverse is numerically unstable. This reduces the precision of parameter estimates and inflates standard errors.\nBinary and Categorical Variables Dummy Variables Dummy variables are binary (0/1) variables that encode group membership.\nExample: Binary Feature\nHouse has a garden: garden = 1 House doesn't have a garden: garden = 0 Encoding Categorical Variables For a categorical variable with \\(K\\) categories, create \\(K-1\\) dummy variables.\nExample: Foundation Type\nSuppose Foundation has 5 categories: Brick, Cinder Block, Slab, Stone, Wood.\nCreate 4 dummy variables:\nFoundation_CinderBlock: 1 if Cinder Block, 0 otherwise Foundation_Slab: 1 if Slab, 0 otherwise Foundation_Stone: 1 if Stone, 0 otherwise Foundation_Wood: 1 if Wood, 0 otherwise Reference category: Brick (when all dummies are 0)\nThe coefficient for each dummy represents the difference in mean response compared to the reference category, holding other variables constant.\nWhy \\(K-1\\) and not \\(K\\)? Including all \\(K\\) dummies would create perfect multicollinearity with the intercept, making \\(X^T X\\) singular.\nPredictor Transformations and Interactions Feature engineering is the creation of new predictors by applying transformations to the original variables.\nCommon Transformations \\[ f_1(X) = \\sqrt{x_1} \\qquad f_2(X) = \\log(x_2) \\qquad f_3(X) = \\sin(x_3) \\]Benefits:\nCapture nonlinear relationships Improve model fit Address skewness or non-constant variance in predictors Interaction Terms An interaction between two variables captures how the effect of one variable depends on the level of another.\n\\[ f(X) = x_1 \\times x_2 \\]Example:\n\\[ Y = a_0 + a_1 \\, \\text{Size} + a_2 \\, \\text{HasGarden} + a_3 \\, (\\text{Size} \\times \\text{HasGarden}) + \\varepsilon \\]The coefficient \\(a_3\\) tells us how much the effect of Size on Price changes when the house has a garden.\nPolynomial Regression Polynomial regression is a special case of multiple linear regression using power functions as transformations:\n\\[ f_k(x) = x^k \\]For a polynomial of degree \\(p\\):\n\\[ Y = a_0 + a_1 x + a_2 x^2 + \\ldots + a_p x^p + \\varepsilon \\]Key point: Although the model is nonlinear in \\(x\\), it is still linear in the parameters \\(a_0, a_1, \\ldots, a_p\\), so we can use OLS.\nExample: Quadratic Fit \\[ Y = a_0 + a_1 x + a_2 x^2 + \\varepsilon \\] Warning: High-degree polynomials can lead to overfitting. Always validate on held-out data.\nSummary In this lesson we covered:\n✅ Multiple regression with multiple predictors ✅ Matrix notation and the design matrix ✅ Matrix operations (multiplication, transpose, inverse, norms) ✅ Normal equation for solving OLS ✅ Dummy variables for categorical predictors ✅ Feature engineering: transformations and interactions ✅ Polynomial regression for nonlinear relationships Next: We'll explore model selection and regularization — hypothesis testing for coefficients, choosing the best predictors, dealing with collinearity, and regularization techniques like Ridge and Lasso.\n","link":"https://jawad.dev/docs/ml/lm/4_multiple_regression_and_feature_engineering/","section":"docs","title":"Multiple Regression and Feature Engineering"},{"body":" Inference in Regression Under the assumption of normality of the errors \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\), we can perform statistical inference about the coefficients.\nDistribution of Coefficient Estimators From the normal equation \\(\\hat{a} = (X^T X)^{-1} X^T Y\\), we have:\n\\[ E(\\hat{a}) = a \\]\\[ \\text{VAR}(\\hat{a}) = \\sigma^2 (X^T X)^{-1} \\]Therefore:\n\\[ \\hat{a} \\sim \\mathcal{N}(a, \\sigma^2 (X^T X)^{-1}) \\] Estimating the Error Variance The variance of the residuals can be approximated using the residual sum of squares:\n\\[ \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n} (\\hat{\\varepsilon}_i)^2}{n - p} = \\frac{\\|e\\|^2}{n - p} \\]where \\(p\\) is the number of parameters (including intercept).\nStatistical Tests for Coefficients Hypothesis Test for Individual Coefficients To test whether a coefficient \\(a_j\\) is significantly different from zero:\nHypotheses:\n\\(H_0\\): \\(a_j = 0\\) (coefficient has no effect) \\(H_1\\): \\(a_j \\neq 0\\) (coefficient is significant) Test statistic:\nUnder \\(H_0\\), the t-statistic follows a Student's t-distribution with \\(n - p\\) degrees of freedom:\n\\[ t_j = \\frac{\\hat{a}_j}{s_{\\hat{a}_j}} \\sim t_{n-p} \\]where \\(s_{\\hat{a}_j}\\) is the standard error of \\(\\hat{a}_j\\), obtained from the diagonal of:\n\\[ \\text{VAR}(\\hat{a}) = \\hat{\\sigma}^2 (X^T X)^{-1} \\]Decision rule:\nIf \\(|t_j| \u003e t_{\\alpha/2, n-p}\\), reject \\(H_0\\) at significance level \\(\\alpha\\) Typically use \\(\\alpha = 0.05\\) (5%) or \\(\\alpha = 0.01\\) (1%) Confidence Intervals Confidence Interval for Coefficients Based on the distribution of \\(\\hat{a}_j\\), a \\((1-\\alpha)\\) confidence interval for \\(a_j\\) is:\n\\[ \\hat{a}_j \\in \\left[ \\hat{a}_j - s_{\\hat{a}_j} \\, t_{\\alpha/2, n-p} \\;,\\; \\hat{a}_j + s_{\\hat{a}_j} \\, t_{\\alpha/2, n-p} \\right] \\]where \\(t_{\\alpha/2, n-p}\\) is the critical value from the Student's t-distribution.\nInterpretation: We are \\((1-\\alpha) \\times 100\\%\\) confident that the true parameter \\(a_j\\) lies within this interval.\nConfidence Interval for Predictions For a new observation with predictors \\(x_{\\text{new}}\\), the predicted value is:\n\\[ \\hat{y}_{\\text{new}} = x_{\\text{new}}^T \\hat{a} \\]The variance of the prediction is:\n\\[ \\text{VAR}(\\hat{y}_{\\text{new}}) = \\sigma^2 x_{\\text{new}}^T (X^T X)^{-1} x_{\\text{new}} \\]A \\((1-\\alpha)\\) confidence interval for the prediction is:\n\\[ \\hat{y}_{\\text{new}} \\in \\left[ \\hat{y}_{\\text{new}} - s_{\\hat{y}} \\, t_{\\alpha/2, n-p} \\;,\\; \\hat{y}_{\\text{new}} + s_{\\hat{y}} \\, t_{\\alpha/2, n-p} \\right] \\]where:\n\\[ s_{\\hat{y}} = \\hat{\\sigma} \\sqrt{x_{\\text{new}}^T (X^T X)^{-1} x_{\\text{new}}} \\] Note: In simple linear regression, confidence bands have a hyperbolic shape, wider at the extremes of the data range.\nAdjusted R² The Problem with R² \\(R^2\\) is at least weakly increasing with the number of regressors:\n\\[ R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} \\]Adding more predictors (even irrelevant ones) can never decrease \\(R^2\\). This makes it unsuitable for comparing models with different numbers of variables.\nAdjusted R² The adjusted R² penalizes model complexity:\n\\[ \\bar{R}^2 = 1 - \\frac{\\text{SSE}/(n-p)}{\\text{SST}/(n-1)} = 1 - (1 - R^2) \\frac{n-1}{n-p} \\]Properties:\nCan decrease if adding a predictor doesn't improve fit enough to justify the extra parameter More appropriate for model comparison Still has limitations (not a likelihood-based criterion) The F-Distribution The F-distribution with parameters \\(d_1\\) and \\(d_2\\) arises as the ratio of two independent chi-squared variables:\nIf \\(S_1 \\sim \\chi^2_{d_1}\\) and \\(S_2 \\sim \\chi^2_{d_2}\\), then:\n\\[ F = \\frac{S_1 / d_1}{S_2 / d_2} \\sim F(d_1, d_2) \\] Properties:\nAlways non-negative Right-skewed Shape depends on both \\(d_1\\) and \\(d_2\\) The F-Test for Regression Overall Significance Test (Simple Regression) The F-test of overall significance compares a model with predictors to an intercept-only model.\nHypotheses:\n\\(H_0\\): \\(a = 0\\) (the model is no better than the mean) \\(H_1\\): \\(a \\neq 0\\) (the model has explanatory power) Test statistic:\n\\[ F_{\\text{obs}} = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{\\text{SSR}/1}{\\text{SSE}/(n-2)} \\]where:\nSSR (Sum of Squares Regression) = \\(\\sum_{i=1}^{n} (\\hat{Y}_i - \\bar{Y})^2\\) SSE (Sum of Squares Error) = \\(\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\\) MSR (Mean Square Regression) = \\(\\text{SSR} / 1\\) MSE (Mean Square Error) = \\(\\text{SSE} / (n-2)\\) Under \\(H_0\\):\n\\[ F_{\\text{obs}} \\sim F(1, n-2) \\] Relationship to R² For simple linear regression, there's a direct relationship:\n\\[ F_{\\text{obs}} = \\frac{R^2}{1 - R^2} \\cdot (n - 2) \\] F-Test for Nested Models Comparing Two Models Consider two nested models:\nModel 1 (restricted): \\(p_1\\) parameters Model 2 (unrestricted): \\(p_2\\) parameters, where \\(p_2 \u003e p_1\\) Model 1 is \u0026quot;nested\u0026quot; within Model 2 if every model in 1 can be represented by some choice of parameters in 2.\nHypotheses:\n\\(H_0\\): Model 2 does not provide a significantly better fit than Model 1 \\(H_1\\): Model 2 provides a significantly better fit Test statistic:\n\\[ F = \\frac{(\\text{SSE}_1 - \\text{SSE}_2) / (p_2 - p_1)}{\\text{SSE}_2 / (n - p_2)} \\sim F(p_2 - p_1, n - p_2) \\]Interpretation:\nLarge \\(F\\) → Reject \\(H_0\\) → The additional predictors improve the model Small \\(F\\) → Fail to reject \\(H_0\\) → The additional predictors are not justified Model Selection Methods When we have many potential predictors, we need systematic methods to select which ones to include.\n1. Backward Elimination Algorithm:\nStart with the full model (all predictors) Choose a significance level to stay (SLS), e.g., 0.05 Fit the model and find the predictor with the largest p-value If \\(p\\text{-value} \u003e \\text{SLS}\\), remove that predictor Refit the model with remaining predictors Repeat steps 3-5 until all predictors have \\(p\\text{-value} \\leq \\text{SLS}\\) Pros: Simple, considers interactions among predictors Cons: Can miss important variables if they're only significant in combination with others\n2. Forward Selection Algorithm:\nStart with the intercept-only model (no predictors) Choose a significance level to enter (SLE), e.g., 0.05 Fit all simple models (one predictor each) Find the predictor with the smallest p-value If \\(p\\text{-value} \u003c \\text{SLE}\\), add that predictor Among remaining predictors, fit two-variable models including the selected variable Repeat steps 4-6 until no additional variables have \\(p\\text{-value} \u003c \\text{SLE}\\) Pros: Computationally efficient for large predictor sets Cons: Once a variable is added, it stays (can't remove it later)\n3. Stepwise Regression Algorithm: Combines forward selection and backward elimination:\nStart like forward selection After each new variable is added, perform backward elimination on all variables Continue until no variables can be added or removed Pros: More flexible than forward or backward alone Cons: Still greedy, no guarantee of finding the optimal model\nInformation Criteria Akaike Information Criterion (AIC) \\[ \\text{AIC} = 2p - 2\\ln(L) \\]where:\n\\(p\\) is the number of parameters \\(L\\) is the maximized likelihood For linear regression:\n\\[ \\text{AIC} = n \\ln\\left(\\frac{\\text{SSE}}{n}\\right) + 2p \\]Lower AIC is better (balances fit and complexity)\nBayesian Information Criterion (BIC) \\[ \\text{BIC} = p \\ln(n) - 2\\ln(L) \\]For linear regression:\n\\[ \\text{BIC} = n \\ln\\left(\\frac{\\text{SSE}}{n}\\right) + p \\ln(n) \\]Lower BIC is better\nBIC vs AIC:\nBIC penalizes complexity more heavily than AIC (especially for large \\(n\\)) BIC tends to select simpler models than AIC Both are useful; often compare multiple criteria Dealing with Collinearity What is Collinearity? Collinearity occurs when predictors are highly correlated with each other.\nPerfect collinearity: One predictor is an exact linear combination of others\n\\(X^T X\\) is singular (not invertible) Parameters are non-identifiable High collinearity (but not perfect):\n\\(X^T X\\) is technically invertible but numerically unstable Coefficient estimates have large standard errors Small changes in data cause large changes in coefficients Detecting Collinearity Variance Inflation Factor (VIF):\nFor predictor \\(j\\):\n\\[ \\text{VIF}_j = \\frac{1}{1 - R_j^2} \\]where \\(R_j^2\\) is the \\(R^2\\) from regressing \\(X_j\\) on all other predictors.\nRule of thumb:\n\\(\\text{VIF} \u003e 5\\): Moderate collinearity \\(\\text{VIF} \u003e 10\\): Severe collinearity Solutions to Collinearity Remove one of the correlated predictors Combine predictors (e.g., create an average or principal component) Regularization methods: Ridge, Lasso, Elastic Net Collect more data (if possible) Regularization: Ridge and Lasso When \\(p\\) is large or predictors are highly correlated, regularization can help.\nRidge Regression Objective:\n\\[ \\min_a \\left\\{ \\|Y - Xa\\|^2 + \\lambda \\|a\\|^2 \\right\\} \\] Adds an \\(\\ell_2\\) penalty on coefficient magnitudes Shrinks coefficients toward zero (but never exactly zero) Solution: \\(\\hat{a}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T Y\\) Lasso Regression Objective:\n\\[ \\min_a \\left\\{ \\|Y - Xa\\|^2 + \\lambda \\|a\\|_1 \\right\\} \\] Adds an \\(\\ell_1\\) penalty on coefficient magnitudes Can set some coefficients exactly to zero (variable selection) No closed-form solution (requires optimization algorithms) Choosing \\(\\lambda\\): Use cross-validation to select the regularization parameter that minimizes prediction error on held-out data.\nSummary In this lesson we covered:\n✅ Inference in regression: hypothesis tests and confidence intervals for coefficients ✅ Adjusted R² to account for model complexity ✅ F-distribution and F-tests for overall significance and nested model comparison ✅ Model selection methods: backward elimination, forward selection, stepwise regression ✅ Information criteria: AIC and BIC ✅ Collinearity: detection (VIF) and solutions ✅ Regularization: Ridge and Lasso for high-dimensional problems Next: We'll extend beyond normal linear regression to Generalized Linear Models (GLM) and logistic regression for classification problems.\n","link":"https://jawad.dev/docs/ml/lm/5_model_selection_and_regularization/","section":"docs","title":"Model Selection and Regularization"},{"body":" Generalized Linear Models (GLM) The generalized linear model (GLM) is a generalization of ordinary linear regression that allows the response variable to have distributions other than the normal distribution.\nThe Three Components of a GLM A GLM consists of three elements:\n1. Exponential Family of Probability Distributions The response variable \\(Y\\) follows a distribution from the exponential family:\nNormal distribution (ordinary linear regression) Bernoulli distribution (logistic regression) Poisson distribution (count data) Exponential distribution (survival/time-to-event data) Gamma distribution (positive continuous data with skewness) 2. Linear Predictor \\[ \\eta = Xa = a_0 + a_1 X_1 + a_2 X_2 + \\ldots + a_p X_p \\]This is the same linear combination of predictors as in ordinary regression.\n3. Link Function A link function \\(g\\) connects the expected value \\(\\mu = E(Y)\\) to the linear predictor:\n\\[ g(\\mu) = \\eta \\]or equivalently:\n\\[ E(Y) = \\mu = g^{-1}(\\eta) \\] GLM Formula The generalized linear model is defined by:\n\\[ E(Y) = g^{-1}(Xa) \\]For ordinary linear regression:\nDistribution: Normal Link function: \\(g(x) = x\\) (identity link) Result: \\(E(Y) = Xa\\) Common Link Functions Different response distributions have corresponding canonical link functions:\nDistribution Support Link Function \\(g(\\mu)\\) Inverse Link \\(g^{-1}(\\eta)\\) Use Case Normal \\(\\mathbb{R}\\) \\(\\mu\\) (identity) \\(\\eta\\) Continuous responses Bernoulli \\(\\{0, 1\\}\\) \\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\) (logit) \\(\\frac{e^\\eta}{1+e^\\eta}\\) Binary classification Poisson \\(\\mathbb{N}\\) \\(\\log(\\mu)\\) \\(e^\\eta\\) Count data Gamma \\(\\mathbb{R}^+\\) \\(\\frac{1}{\\mu}\\) (inverse) \\(\\frac{1}{\\eta}\\) Positive continuous (e.g., insurance claims) Parameter Estimation: Maximum Likelihood In GLMs, the unknown parameters \\(a\\) are typically estimated using maximum likelihood estimation (MLE):\n\\[ \\hat{a} = \\arg\\max_a \\, L(a \\mid Y, X) = \\arg\\max_a \\sum_{i=1}^{n} \\log p(y_i \\mid x_i, a) \\]Unlike ordinary regression (which has a closed-form solution), GLMs generally require iterative optimization algorithms:\nNewton-Raphson method Iteratively Reweighted Least Squares (IRLS) Logistic Regression Logistic regression is used when the response variable \\(Y\\) is binary (taking values 0 or 1).\nBinary Response Distribution When \\(Y_i \\in \\{0, 1\\}\\), we model it as a Bernoulli random variable:\n\\[ Y_i \\sim \\text{Bernoulli}(p_i) \\]where \\(p_i = P(Y_i = 1)\\) is the probability of \u0026quot;success\u0026quot; for observation \\(i\\).\nThe expected value is:\n\\[ E(Y_i) = \\mu_i = p_i \\] The Logit Link Function The logit (log-odds) link function is the canonical link for logistic regression:\n\\[ g(\\mu) = \\log\\left(\\frac{\\mu}{1 - \\mu}\\right) = \\log\\left(\\frac{p}{1 - p}\\right) \\]This maps probabilities from \\([0, 1]\\) to the entire real line \\((-\\infty, +\\infty)\\).\nInverse Link: The Logistic Function Inverting the logit gives the logistic function (sigmoid):\n\\[ \\mu = g^{-1}(\\eta) = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)} = \\frac{1}{1 + \\exp(-\\eta)} \\]This maps the linear predictor \\(\\eta\\) back to a probability in \\([0, 1]\\).\nLogistic Regression Model For a binary response, the logistic regression model is:\n\\[ \\log\\left(\\frac{p_i}{1 - p_i}\\right) = a_0 + a_1 X_{i1} + a_2 X_{i2} + \\ldots + a_p X_{ip} \\]Equivalently:\n\\[ p_i = P(Y_i = 1 \\mid X_i) = \\frac{\\exp(a_0 + a_1 X_{i1} + \\ldots + a_p X_{ip})}{1 + \\exp(a_0 + a_1 X_{i1} + \\ldots + a_p X_{ip})} \\]Interpretation of coefficients:\n\\(a_j \u003e 0\\): Increasing \\(X_j\\) increases the log-odds (and thus the probability) of \\(Y = 1\\) \\(a_j \u003c 0\\): Increasing \\(X_j\\) decreases the log-odds of \\(Y = 1\\) \\(\\exp(a_j)\\): The odds ratio associated with a one-unit increase in \\(X_j\\) Maximum Likelihood for Logistic Regression Likelihood Function For binary data, the likelihood is:\n\\[ L(a) = \\prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i} \\] Log-Likelihood Taking the logarithm:\n\\[ \\ell(a) = \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right] \\]Substituting \\(p_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}\\) where \\(\\eta_i = x_i^T a\\):\n\\[ \\ell(a) = \\sum_{i=1}^{n} \\left[ y_i \\eta_i - \\log(1 + \\exp(\\eta_i)) \\right] \\] Negative Log-Likelihood (Loss Function) In practice, we minimize the negative log-likelihood:\n\\[ \\text{NLL}(a) = -\\ell(a) = -\\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right] \\]This is also known as the binary cross-entropy loss.\nOptimization: There is no closed-form solution. We use iterative algorithms:\nGradient descent Newton-Raphson IRLS (Iteratively Reweighted Least Squares) Interpretation and Decision Boundary Odds and Odds Ratio The odds of \\(Y = 1\\) are:\n\\[ \\text{Odds} = \\frac{p}{1 - p} = \\exp(\\eta) = \\exp(a_0 + a_1 X_1 + \\ldots + a_p X_p) \\]For a one-unit increase in \\(X_j\\):\n\\[ \\text{Odds Ratio} = \\exp(a_j) \\]Example: If \\(\\exp(a_1) = 2.5\\), then a one-unit increase in \\(X_1\\) multiplies the odds of \\(Y = 1\\) by 2.5.\nDecision Boundary To classify a new observation, we typically use a threshold (e.g., \\(p = 0.5\\)):\n\\[ \\hat{Y} = \\begin{cases} 1 \u0026 \\text{if } p \u003e 0.5 \\\\ 0 \u0026 \\text{otherwise} \\end{cases} \\]The decision boundary is where \\(p = 0.5\\), which occurs when:\n\\[ \\eta = a_0 + a_1 X_1 + \\ldots + a_p X_p = 0 \\]For two predictors, this defines a line in the feature space.\nModel Evaluation for Logistic Regression Deviance The deviance is a measure of model fit:\n\\[ D = -2 \\ell(a) \\]Lower deviance indicates better fit.\nConfusion Matrix and Metrics For classification tasks, we use:\nPredicted 0 Predicted 1 Actual 0 TN (True Neg) FP (False Pos) Actual 1 FN (False Neg) TP (True Pos) Metrics:\nAccuracy: \\(\\frac{TP + TN}{TP + TN + FP + FN}\\) Precision: \\(\\frac{TP}{TP + FP}\\) Recall (Sensitivity): \\(\\frac{TP}{TP + FN}\\) Specificity: \\(\\frac{TN}{TN + FP}\\) F1-Score: Harmonic mean of precision and recall ROC Curve and AUC The ROC curve (Receiver Operating Characteristic) plots True Positive Rate vs False Positive Rate at various threshold settings.\nAUC (Area Under the Curve):\n\\(AUC = 1\\): Perfect classifier \\(AUC = 0.5\\): Random classifier \\(AUC \u003e 0.8\\): Generally considered good Regularization for Logistic Regression Like linear regression, logistic regression can benefit from regularization when dealing with many predictors or collinearity.\nL2 Regularization (Ridge) \\[ \\text{NLL}(a) + \\lambda \\sum_{j=1}^{p} a_j^2 \\] L1 Regularization (Lasso) \\[ \\text{NLL}(a) + \\lambda \\sum_{j=1}^{p} |a_j| \\]Benefits:\nPrevents overfitting Can perform variable selection (Lasso) Improves generalization to new data Multiclass Logistic Regression For \\(K \u003e 2\\) classes, we use softmax regression (multinomial logistic regression).\nThe probability that observation \\(i\\) belongs to class \\(k\\) is:\n\\[ p_{ik} = P(Y_i = k \\mid X_i) = \\frac{\\exp(\\eta_{ik})}{\\sum_{j=1}^{K} \\exp(\\eta_{ij})} \\]where \\(\\eta_{ik} = a_k^T X_i\\) for each class \\(k\\).\nTraining: Minimize the categorical cross-entropy loss:\n\\[ \\text{Loss} = -\\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{1}(y_i = k) \\log(p_{ik}) \\] Summary In this lesson we covered:\n✅ Generalized Linear Models (GLM): extending beyond normal distributions ✅ Three components of a GLM: distribution, linear predictor, link function ✅ Logistic regression: binary classification using the logit link ✅ Maximum likelihood estimation: solving the negative log-likelihood ✅ Interpretation: odds, odds ratios, decision boundaries ✅ Model evaluation: confusion matrix, accuracy, precision, recall, ROC/AUC ✅ Regularization: Ridge and Lasso for logistic regression ✅ Multiclass extension: softmax regression This completes the linear models series! You now have a solid foundation in:\nSimple and multiple linear regression Inference and diagnostics Feature engineering and polynomial regression Model selection and regularization GLMs and logistic regression for classification Further Reading Generalized Additive Models (GAMs): Non-parametric extensions of GLMs Survival Analysis: Cox proportional hazards model Count Data: Poisson and negative binomial regression Hierarchical/Mixed Effects Models: Accounting for grouped/nested data structure ","link":"https://jawad.dev/docs/ml/lm/6_generalized_linear_models_and_logistic/","section":"docs","title":"Generalized Linear Models (GLM) and Logistic Regression"},{"body":"This annex provides detailed mathematical derivations for the key results in linear models.\nAnnex A: OLS Solution for Simple Linear Regression Problem Statement For simple linear regression:\n\\[\rY_i = a X_i + b + \\varepsilon_i\r\\]We want to minimize the sum of squared residuals:\n\\[\r\\text{SSE}(a, b) = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} (Y_i - (aX_i + b))^2\r\\] Derivation Step 1: Expand the objective function\n\\[\r\\text{SSE}(a, b) = \\sum_{i=1}^{n} (Y_i - aX_i - b)^2\r\\]Step 2: Take partial derivatives\nPartial derivative with respect to \\(a\\):\n\\[\r\\frac{\\partial \\text{SSE}}{\\partial a} = \\sum_{i=1}^{n} 2(Y_i - aX_i - b)(-X_i) = -2 \\sum_{i=1}^{n} X_i(Y_i - aX_i - b)\r\\]\\[\r= -2 \\left[ \\sum_{i=1}^{n} X_i Y_i - a \\sum_{i=1}^{n} X_i^2 - b \\sum_{i=1}^{n} X_i \\right]\r\\]Partial derivative with respect to \\(b\\):\n\\[\r\\frac{\\partial \\text{SSE}}{\\partial b} = \\sum_{i=1}^{n} 2(Y_i - aX_i - b)(-1) = -2 \\sum_{i=1}^{n} (Y_i - aX_i - b)\r\\]\\[\r= -2 \\left[ \\sum_{i=1}^{n} Y_i - a \\sum_{i=1}^{n} X_i - nb \\right]\r\\]Step 3: Set partial derivatives to zero\nSetting \\(\\frac{\\partial \\text{SSE}}{\\partial b} = 0\\):\n\\[\r\\sum_{i=1}^{n} Y_i - a \\sum_{i=1}^{n} X_i - nb = 0\r\\]\\[\rnb = \\sum_{i=1}^{n} Y_i - a \\sum_{i=1}^{n} X_i\r\\]\\[\rb = \\frac{1}{n} \\sum_{i=1}^{n} Y_i - a \\cdot \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\bar{Y} - a\\bar{X}\r\\]This gives us:\n\\[\r\\boxed{\\hat{b} = \\bar{Y} - \\hat{a}\\bar{X}}\r\\]Setting \\(\\frac{\\partial \\text{SSE}}{\\partial a} = 0\\) and substituting \\(b = \\bar{Y} - a\\bar{X}\\):\n\\[\r\\sum_{i=1}^{n} X_i Y_i - a \\sum_{i=1}^{n} X_i^2 - (\\bar{Y} - a\\bar{X}) \\sum_{i=1}^{n} X_i = 0\r\\]\\[\r\\sum_{i=1}^{n} X_i Y_i - a \\sum_{i=1}^{n} X_i^2 - \\bar{Y} \\sum_{i=1}^{n} X_i + a\\bar{X} \\sum_{i=1}^{n} X_i = 0\r\\]\\[\r\\sum_{i=1}^{n} X_i Y_i - \\bar{Y} \\sum_{i=1}^{n} X_i = a \\left( \\sum_{i=1}^{n} X_i^2 - \\bar{X} \\sum_{i=1}^{n} X_i \\right)\r\\]Step 4: Simplify using centered forms\nNote that \\(\\sum_{i=1}^{n} X_i = n\\bar{X}\\), so:\n\\[\r\\sum_{i=1}^{n} X_i Y_i - n\\bar{X}\\bar{Y} = a \\left( \\sum_{i=1}^{n} X_i^2 - n\\bar{X}^2 \\right)\r\\]Recognizing covariance and variance:\n\\[\r\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) = \\sum_{i=1}^{n} X_i Y_i - n\\bar{X}\\bar{Y}\r\\]\\[\r\\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum_{i=1}^{n} X_i^2 - n\\bar{X}^2\r\\]Therefore:\n\\[\r\\boxed{\\hat{a} = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}}\r\\] Annex B: Normal Equation for Multiple Regression Problem Statement For multiple regression in matrix form:\n\\[\rY = Xa + E\r\\]We want to minimize:\n\\[\r\\text{SSE}(a) = \\|Y - Xa\\|^2 = (Y - Xa)^T(Y - Xa)\r\\] Derivation Step 1: Expand the objective function\n\\[\r\\text{SSE}(a) = (Y - Xa)^T(Y - Xa)\r\\]\\[\r= Y^T Y - Y^T Xa - a^T X^T Y + a^T X^T X a\r\\]Since \\(Y^T Xa\\) and \\(a^T X^T Y\\) are scalars and equal:\n\\[\r= Y^T Y - 2a^T X^T Y + a^T X^T X a\r\\]Step 2: Take the derivative with respect to \\(a\\)\nUsing matrix calculus rules:\n\\(\\frac{\\partial}{\\partial a}(a^T c) = c\\) for constant vector \\(c\\) \\(\\frac{\\partial}{\\partial a}(a^T A a) = 2Aa\\) for symmetric matrix \\(A\\) \\[\r\\frac{\\partial \\text{SSE}}{\\partial a} = -2X^T Y + 2X^T X a\r\\]Step 3: Set derivative to zero\n\\[\r-2X^T Y + 2X^T X a = 0\r\\]\\[\rX^T X a = X^T Y\r\\]Step 4: Solve for \\(a\\)\nAssuming \\(X^T X\\) is invertible (i.e., \\(X\\) has full column rank):\n\\[\r\\boxed{\\hat{a} = (X^T X)^{-1} X^T Y}\r\\] Properties of the Normal Equation Unbiasedness:\n\\[\rE(\\hat{a}) = E\\left[ (X^T X)^{-1} X^T Y \\right] = E\\left[ (X^T X)^{-1} X^T (Xa + E) \\right]\r\\]\\[\r= (X^T X)^{-1} X^T X a + (X^T X)^{-1} X^T E(E) = a + 0 = a\r\\]Variance:\n\\[\r\\text{Var}(\\hat{a}) = \\text{Var}\\left[ (X^T X)^{-1} X^T Y \\right]\r\\]Since \\(Y = Xa + E\\):\n\\[\r\\hat{a} = (X^T X)^{-1} X^T (Xa + E) = a + (X^T X)^{-1} X^T E\r\\]\\[\r\\hat{a} - a = (X^T X)^{-1} X^T E\r\\]\\[\r\\text{Var}(\\hat{a}) = E\\left[ (\\hat{a} - a)(\\hat{a} - a)^T \\right]\r\\]\\[\r= E\\left[ (X^T X)^{-1} X^T E E^T X (X^T X)^{-1} \\right]\r\\]Under the assumption \\(E(E E^T) = \\sigma^2 I\\):\n\\[\r= (X^T X)^{-1} X^T (\\sigma^2 I) X (X^T X)^{-1}\r\\]\\[\r= \\sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1}\r\\]\\[\r\\boxed{\\text{Var}(\\hat{a}) = \\sigma^2 (X^T X)^{-1}}\r\\] Annex C: Maximum Likelihood for Logistic Regression Problem Statement For logistic regression with binary response \\(Y_i \\in \\{0, 1\\}\\):\n\\[\rP(Y_i = 1 \\mid X_i) = p_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(-\\eta_i)}\r\\]where \\(\\eta_i = x_i^T a\\).\nWe want to find the maximum likelihood estimator \\(\\hat{a}\\).\nLikelihood Function For a single observation:\n\\[\rP(Y_i = y_i \\mid X_i) = p_i^{y_i} (1 - p_i)^{1 - y_i}\r\\]For \\(n\\) independent observations:\n\\[\rL(a) = \\prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}\r\\] Log-Likelihood Taking logarithms:\n\\[\r\\ell(a) = \\log L(a) = \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\r\\]Step 1: Express in terms of \\(\\eta_i\\)\n\\[\rp_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(-\\eta_i)}\r\\]\\[\r1 - p_i = \\frac{1}{1 + \\exp(\\eta_i)}\r\\]\\[\r\\log(p_i) = \\log\\left(\\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}\\right) = \\eta_i - \\log(1 + \\exp(\\eta_i))\r\\]\\[\r\\log(1 - p_i) = \\log\\left(\\frac{1}{1 + \\exp(\\eta_i)}\\right) = -\\log(1 + \\exp(\\eta_i))\r\\]Step 2: Substitute into log-likelihood\n\\[\r\\ell(a) = \\sum_{i=1}^{n} \\left[ y_i (\\eta_i - \\log(1 + \\exp(\\eta_i))) + (1 - y_i)(-\\log(1 + \\exp(\\eta_i))) \\right]\r\\]\\[\r= \\sum_{i=1}^{n} \\left[ y_i \\eta_i - y_i \\log(1 + \\exp(\\eta_i)) - \\log(1 + \\exp(\\eta_i)) + y_i \\log(1 + \\exp(\\eta_i)) \\right]\r\\]\\[\r= \\sum_{i=1}^{n} \\left[ y_i \\eta_i - \\log(1 + \\exp(\\eta_i)) \\right]\r\\]\\[\r\\boxed{\\ell(a) = \\sum_{i=1}^{n} \\left[ y_i x_i^T a - \\log(1 + \\exp(x_i^T a)) \\right]}\r\\] Negative Log-Likelihood (Loss Function) To minimize (equivalent to maximizing \\(\\ell(a)\\)):\n\\[\r\\boxed{\\text{NLL}(a) = -\\ell(a) = \\sum_{i=1}^{n} \\left[ \\log(1 + \\exp(x_i^T a)) - y_i x_i^T a \\right]}\r\\]Alternatively, in the original probability form:\n\\[\r\\boxed{\\text{NLL}(a) = -\\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]}\r\\]This is the binary cross-entropy loss.\nGradient of the Negative Log-Likelihood To solve using gradient descent or Newton-Raphson, we need:\n\\[\r\\frac{\\partial \\text{NLL}}{\\partial a_j} = \\sum_{i=1}^{n} (p_i - y_i) x_{ij}\r\\]In vector form:\n\\[\r\\nabla_a \\text{NLL} = X^T (p - y)\r\\]where:\n\\(X\\) is the \\(n \\times p\\) design matrix \\(p = (p_1, \\ldots, p_n)^T\\) is the vector of predicted probabilities \\(y = (y_1, \\ldots, y_n)^T\\) is the vector of observed responses Hessian (Second Derivative Matrix) For Newton-Raphson optimization:\n\\[\rH = \\frac{\\partial^2 \\text{NLL}}{\\partial a \\partial a^T} = X^T W X\r\\]where \\(W\\) is a diagonal matrix with:\n\\[\rW_{ii} = p_i(1 - p_i)\r\\] Newton-Raphson Update The iterative update rule is:\n\\[\ra^{(t+1)} = a^{(t)} - (X^T W X)^{-1} X^T (p - y)\r\\]This is also known as Iteratively Reweighted Least Squares (IRLS) because the weights \\(W\\) change at each iteration.\nAnnex D: Derivation of Coefficient Variance Distribution of the Estimator Starting from the normal equation:\n\\[\r\\hat{a} = (X^T X)^{-1} X^T Y\r\\]Substituting \\(Y = Xa + E\\):\n\\[\r\\hat{a} = (X^T X)^{-1} X^T (Xa + E) = a + (X^T X)^{-1} X^T E\r\\] Expected Value \\[\rE(\\hat{a}) = E\\left[ a + (X^T X)^{-1} X^T E \\right] = a + (X^T X)^{-1} X^T E(E) = a\r\\](Since \\(E(E) = 0\\))\nThus, \\(\\hat{a}\\) is unbiased.\nVariance-Covariance Matrix \\[\r(\\hat{a} - a)(\\hat{a} - a)^T = [(X^T X)^{-1} X^T E][(X^T X)^{-1} X^T E]^T\r\\]\\[\r= (X^T X)^{-1} X^T E E^T X (X^T X)^{-1}\r\\]Taking expectation:\n\\[\rE[(\\hat{a} - a)(\\hat{a} - a)^T] = (X^T X)^{-1} X^T E(E E^T) X (X^T X)^{-1}\r\\]Under the assumption \\(E(E E^T) = \\sigma^2 I_n\\):\n\\[\r= (X^T X)^{-1} X^T (\\sigma^2 I_n) X (X^T X)^{-1}\r\\]\\[\r= \\sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1}\r\\]\\[\r= \\sigma^2 (X^T X)^{-1} I_p (X^T X)^{-1}\r\\]\\[\r\\boxed{\\text{Var}(\\hat{a}) = \\sigma^2 (X^T X)^{-1}}\r\\] Distribution Under Normality If \\(E \\sim \\mathcal{N}(0, \\sigma^2 I)\\), then:\n\\[\r\\boxed{\\hat{a} \\sim \\mathcal{N}\\left(a, \\sigma^2 (X^T X)^{-1}\\right)}\r\\]This result is fundamental for:\nHypothesis testing (t-tests for individual coefficients) Confidence intervals F-tests for nested models Summary These annexes provide the mathematical foundations for:\n✅ OLS estimators for simple linear regression ✅ Normal equation for multiple regression ✅ Maximum likelihood estimation for logistic regression ✅ Distributional properties of coefficient estimators Understanding these derivations helps build intuition for:\nWhy OLS works When normal equations fail (singular \\(X^T X\\)) Why logistic regression requires iterative optimization The role of normality assumptions in inference Recommended Practice: Work through these derivations by hand to solidify your understanding of the mathematical foundations of linear models!\n","link":"https://jawad.dev/docs/ml/lm/7_mathematical_annexes/","section":"docs","title":"Mathematical Annexes"},{"body":"","link":"https://jawad.dev/tags/ai/","section":"tags","title":"AI"},{"body":"","link":"https://jawad.dev/tags/evaluation/","section":"tags","title":"Evaluation"},{"body":"Between May and June 2025, I wrote a series of posts on LinkedIn about the reality of designing, evaluating, and scaling multi-agent AI systems. This article is a direct regrouping and rephrasing of those posts, with no added or removed content — only organized for clarity.\n📌 Table of Contents Evaluation is your main driver The real battle begins in Production Regression Testing and Guardrails: The Hidden Heroes Evaluation inside the Agentic Loop Building for Scale and Automation 1. Evaluation is your main driver Most tech teams have been missing a key point when evaluating AI systems It's not new. Even back in the classical ML days, I noticed this pattern:\nwe stuck to abstract, generic metrics (accuracy, F1-score, etc.) instead of designing domain-specific ones that truly reflect business value.\nToday, I'll go even further:\n👉 𝗕𝘂𝗶𝗹𝗱 𝗰𝗹𝗶𝗲𝗻𝘁-𝘀𝗽𝗲𝗰𝗶𝗳𝗶𝗰 𝗺𝗲𝘁𝗿𝗶𝗰𝘀: metrics that align exactly with your client's goals and values.\nSpoiler: 𝘁𝗵𝗲𝘀𝗲 𝗼𝗳𝘁𝗲𝗻 𝗯𝗼𝗶𝗹 𝗱𝗼𝘄𝗻 𝘁𝗼 𝗺𝗼𝗻𝗲𝘆 💲 𝗥𝗢𝗜.\nConcrete example. 𝗔𝘀𝗸 𝗯𝘂𝘀𝗶𝗻𝗲𝘀𝘀 𝗼𝘄𝗻𝗲𝗿𝘀 𝘄𝗵𝗶𝗰𝗵 𝘀𝗰𝗲𝗻𝗮𝗿𝗶𝗼 𝘁𝗵𝗲𝘆'𝗱 𝗰𝗵𝗼𝗼𝘀𝗲:\n1️⃣ A sales AI that hallucinates 10% of the time, but closes 20% of deals\n2️⃣ A sales AI that hallucinates 1%, but only closes 1%\nThe answer isn't obvious right ? it's not that easy since you do not know the real impact in terme of costs and revenues for each scenario...\nCan LLM-as-a-judge be considered a form of evaluation? As a data scientist, my answer is NO. at least, not in the strict sense of the word.\nHere's why:\nEvaluation without a gold standard is just another opinion.\nOr if you prefer: it's more of an 𝗮𝗻𝗮𝗹𝘆𝘀𝗶𝘀 𝘁𝗵𝗮𝗻 𝗮𝗻 𝗲𝘃𝗮𝗹𝘂𝗮𝘁𝗶𝗼𝗻.\nDon't get me wrong! I'm not saying LLM-as-a-judge is useless. It can provide useful signals to improve your system, 𝗲𝘀𝗽𝗲𝗰𝗶𝗮𝗹𝗹𝘆 𝘄𝗵𝗲𝗻 𝗵𝘂𝗺𝗮𝗻 𝗲𝘃𝗮𝗹𝘂𝗮𝘁𝗶𝗼𝗻𝘀 𝗮𝗿𝗲𝗻'𝘁 𝘀𝗰𝗮𝗹𝗮𝗯𝗹𝗲.\nBut calling it evaluation is misleading, because you're 𝗻𝗼𝘁 𝗰𝗼𝗺𝗽𝗮𝗿𝗶𝗻𝗴 𝘁𝗼 𝗮 𝘀𝗼𝘂𝗿𝗰𝗲 𝗼𝗳 𝘁𝗿𝘂𝘁𝗵.\nAs a data scientist, I've already seen this kind of confusion, especially when discussing unsupervised learning evaluation.\nIf you've worked in data science, you know:\nEvaluation in unsupervised learning is tricky. You often have to 𝘁𝗶𝗲 𝗶𝘁 𝗯𝗮𝗰𝗸 𝘁𝗼 𝗮 𝘀𝘂𝗽𝗲𝗿𝘃𝗶𝘀𝗲𝗱 𝘁𝗮𝘀𝗸 𝗼𝗿 𝗮 𝗯𝘂𝘀𝗶𝗻𝗲𝘀𝘀 𝗼𝘂𝘁𝗰𝗼𝗺𝗲 𝘁𝗼 𝗮𝘀𝘀𝗲𝘀𝘀 𝗶𝘁𝘀 𝘃𝗮𝗹𝘂𝗲.\nSame goes for evaluating text generation.\n𝗦𝗼 𝗵𝗼𝘄 𝗱𝗼 𝘄𝗲 𝗵𝗮𝗻𝗱𝗹𝗲 𝘁𝗵𝗶𝘀 𝗽𝗿𝗼𝗽𝗲𝗿𝗹𝘆?\n✅ Define expected outputs at the data point level so you can apply real metrics.\n✅ Don't ask the LLM to reason, ask it to compare your system's output against a reference.\n✅ Let them help you understand why an output deviated, not just whether it's good or bad.\n✅ Instead of relying on end-to-end judgment, look at what each agent or tool is doing in your system.\nWhat does it mean to evaluate RAG? Is it worth doing? How do we break it down into multiple steps, and what's the most critical part?\nFirst, understand that 𝗥𝗔𝗚 𝗶𝘀𝗻'𝘁 𝗷𝘂𝘀𝘁 𝗮 𝘁𝗲𝘅𝘁-𝗯𝗮𝘀𝗲𝗱 𝘀𝘆𝘀𝘁𝗲𝗺. It can involve various data types and formats integrated into the LLM's context.\nRAG can cover API calls, SQL databases, web searches, and of course vector databases.\nTo control your agent's output quality, you must ensure quality at every process step.\n𝗜𝗻 𝘁𝗵𝗲 𝗮𝗴𝗲𝗻𝘁 𝘀𝗽𝗮𝗰𝗲, 𝗿𝗲𝘁𝗿𝗶𝗲𝘃𝗮𝗹 𝗶𝘀 𝗰𝗿𝗶𝘁𝗶𝗰𝗮𝗹, because it involves multiple sub-steps:\nThe agent builds the query itself. You need to assess whether it's doing this correctly. ex: not misusing database tools or accessing unauthorized data. Evaluate the real targets (entities, lines, documents, or chunks) that should be retrieved and used as context. Then compute metrics like false positives and false negatives. Problems can also come from your knowledge base. You need clean, curated data, especially during the building and testing phases. And ensure you're building efficient context retrieval! If the data you're feeding the LLM is too big (and isn't focused on what needs to be shown to the user) you'll lose your LLM's attention, even with models that support large contexts. 𝗧𝗵𝗲𝗻, 𝗮𝘀𝘀𝗲𝘀𝘀 𝘁𝗵𝗲 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗼𝗻 𝗽𝗮𝗿𝘁. 3 key points to evaluate:\n𝗛𝗮𝗹𝗹𝘂𝗰𝗶𝗻𝗮𝘁𝗶𝗼𝗻 and Non-Based Information: Is the output strictly based on your RAG context? Errors here could come from model quality, prompt design, or lack of guidance. 𝗥𝗲𝗹𝗲𝘃𝗮𝗻𝗰𝘆: Does the generation highlight the right parts of the context? Is it structured as expected, per prompt policy or fine-tuning? 𝗘𝘅𝗵𝗮𝘂𝘀𝘁𝗶𝘃𝗶𝘁𝘆: Is the generated text exhaustive? Be cautious, some filters in the prompt might deliberately exclude parts of the retrieved data. 𝗥𝗔𝗚 𝗶𝘀 𝘁𝗵𝗲 𝗺𝗼𝘀𝘁 𝗶𝗺𝗽𝗼𝗿𝘁𝗮𝗻𝘁 𝗽𝗮𝗿𝘁 𝘁𝗼 𝗲𝘃𝗮𝗹𝘂𝗮𝘁𝗲 𝗶𝗻 𝘆𝗼𝘂𝗿 𝘀𝘆𝘀𝘁𝗲𝗺. It's where most issues originate. Break it down and evaluate each part carefully.\n✅ Study your errors and keep track of them in neat and well-defined automated tests.\n✅ Build a robust test framework with a diverse query set and reference answers.\n✅ Use metrics like precision, recall, and context relevance for retrieval.\n✅ For generation, focus on faithfulness and answer relevance.\n✅ Regularly update your evaluation datasets based on real-world usage.\n✅ Use LLM-based evaluators to scale your assessment efficiently.\n2. The real battle begins in Production Your AI system just hit production. Now comes a new challenge: 𝗻𝗼𝘁 𝗯𝗿𝗲𝗮𝗸𝗶𝗻𝗴 𝗶𝘁 Congratulations! 🎊\nYou've shipped your wonderful AI platform to prod, and it's working pretty well for a first version.\nAnd now comes the hard part: 𝗬𝗼𝘂 𝗻𝗲𝗲𝗱 𝘁𝗼 𝗮𝗱𝗱 𝗮𝗹𝗹 𝘁𝗵𝗲 𝗳𝗲𝗮𝘁𝘂𝗿𝗲𝘀 𝘁𝗵𝗮𝘁 𝘄𝗲𝗿𝗲𝗻'𝘁 𝗶𝗻 𝘆𝗼𝘂𝗿 𝗠𝗩𝗣, the ones that are crucial to keeping your AI services ahead of the competition.\nAnd guess what? It's not going to be a small iteration.\n𝗧𝗵𝗲𝘀𝗲 𝗻𝗲𝘄 𝗳𝗲𝗮𝘁𝘂𝗿𝗲𝘀 𝘄𝗶𝗹𝗹 𝗹𝗶𝗸𝗲𝗹𝘆 𝗿𝗲𝗾𝘂𝗶𝗿𝗲 𝗮 𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗮𝗹 𝗿𝗲𝘃𝗶𝗲𝘄 of your agent orchestration, and even a rethink of your entire multi-agent system architecture (Yes, there are multiple architectures with new ones introduced every week.)\n𝗪𝗲 𝗰𝗮𝗻 𝘀𝗮𝘆 𝗶𝘁:\n𝗬𝗼𝘂𝗿 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻 𝗶𝘀 𝗹𝗲𝗴𝗮𝗰𝘆 𝗶𝗻 𝗹𝗲𝘀𝘀 𝘁𝗵𝗮𝗻 3 𝗺𝗼𝗻𝘁𝗵𝘀. 😅\nThis isn't entirely new…\nBut two things are probably different from your usual tech stack:\n1- 𝗟𝗲𝗴𝗮𝗰𝘆 𝗵𝗮𝗽𝗽𝗲𝗻𝘀 𝗳𝗮𝘀𝘁 𝗶𝗻 𝗔𝗜.\nSome projects become legacy before even hitting production.\nWe've experienced this ourselves. When a framework we were using got a major upgrade, we had no choice but to migrate if we wanted to keep building new features.\n2- 𝗠𝗶𝗴𝗿𝗮𝘁𝗶𝗼𝗻 𝗶𝘀 𝗿𝗶𝘀𝗸𝗶𝗲𝗿.\nYou have less control over your AI platform than with a deterministic system.\nWhich means even small changes can lead to unexpected behaviors.\n𝗔𝗻𝗱 𝗵𝗲𝗿𝗲 𝗰𝗼𝗺𝗲𝘀 𝗿𝗲𝗴𝗿𝗲𝘀𝘀𝗶𝗼𝗻 𝘁𝗲𝘀𝘁𝗶𝗻𝗴.\nRegression tests are your best ally to make sure refactors or architecture changes don't break critical production features.\n✅ Create and maintain dedicated regression scenarios.\n✅ Define metrics that qualify outputs: since you're working with AI, pass/fail isn't enough.\n✅ Automate these tests or every release 𝘄𝗶𝗹𝗹 𝘁𝘂𝗿𝗻 𝗶𝗻𝘁𝗼 𝗱𝗮𝘆𝘀 𝗼𝗳 𝗺𝗮𝗻𝘂𝗮𝗹 𝗤𝗔 𝗮𝗻𝗱 𝗿𝗲𝘁𝗲𝘀𝘁𝗶𝗻𝗴.\nIf you're a startup moving fast in AI with users already rely on your first product:\n𝗶𝗻𝘃𝗲𝘀𝘁 𝗶𝗻 𝗿𝗲𝗴𝗿𝗲𝘀𝘀𝗶𝗼𝗻 𝘁𝗲𝘀𝘁𝗶𝗻𝗴 𝗻𝗼𝘄!\nOtherwise, your roadmap will slow down before it even picks up speed.\nImproving your AI system isn't just about development time. It's far more correlated with the number of real cases processed in production.\nWhy?\nBecause evaluation isn't meaningful until your system has faced a wide range of inputs... and even then, some edge cases won't appear until you're operating at scale.\nHere's what we've observed again and again:\n✅ Complex agent tasks can perform flawlessly in dev, but start breaking down under production load or when inputs deviate even slightly from the original distribution.\n✅ Free text input is unpredictable. No matter how creative your dev team is, users will always surprise you.\n✅ It's not just LLMs : rate limits, data quality issues, and fragile pipelines will all show up once the volume kicks in.\n✅ Even your data pipelines need to be stress tested at scale. You'll spot performance cliffs and resilience gaps only under pressure.\nScaling to production gives your team real signals:\nWhat breaks? Where is the system fragile? What improvements would make the most users happy? With enough volume, your team gets the feedback it needs to iterate faster and with higher confidence.\nScaling isn't a nice-to-have.\nIt's the prerequisite for building reliable and evolving AI systems.\nProduction support and online evaluation are not just ops tasks, they're strategic levers for your team. After months running AI systems 24/7, one thing is clear:\n𝗣𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻 𝗶𝘀 𝘄𝗵𝗲𝗿𝗲 𝘆𝗼𝘂 𝗯𝘂𝗶𝗹𝗱 𝗿𝗲𝗮𝗹 𝗶𝗻𝘁𝘂𝗶𝘁𝗶𝗼𝗻 𝗳𝗼𝗿 𝘆𝗼𝘂𝗿 𝗻𝗲𝘅𝘁 𝘃𝗲𝗿𝘀𝗶𝗼𝗻.\nEvaluation before prod is critical, but watching real user interactions is what sharpens your instincts and helps prioritize what matters.\nHere's how we 𝘁𝘂𝗿𝗻 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗶𝗻𝘁𝗼 𝗮 𝘀𝘂𝗽𝗲𝗿𝗽𝗼𝘄𝗲𝗿:\n🔁 Make it a team-wide habit: Everyone in the team should get 𝗮 𝘁𝗮𝘀𝘁𝗲 𝗼𝗳 𝗿𝗲𝗮𝗹 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻 𝗱𝗮𝘁𝗮. It fuels both creativity and clarity.\n📈 Use it to tune priorities: Support helps reveal what truly matters for the business and what's just \u0026quot;nice to have.\u0026quot;\n𝗪𝗵𝗲𝗻 𝗯𝘂𝗶𝗹𝗱𝗶𝗻𝗴 𝗳𝗮𝘀𝘁, 𝘆𝗼𝘂 𝘄𝗮𝗻𝘁 𝘁𝗼 𝗱𝗲𝗹𝗶𝘃𝗲𝗿 𝘄𝗵𝗮𝘁'𝘀 𝗲𝘀𝘀𝗲𝗻𝘁𝗶𝗮𝗹, and hold off on the rest, especially since priorities often shift between project and production phases.\n💡 Help your team develop good trade-offs: Not every issue needs an immediate fix.\nBut sometimes a 𝗾𝘂𝗶𝗰𝗸 𝗽𝗮𝘁𝗰𝗵 𝗰𝗮𝗻 𝗮𝘃𝗼𝗶𝗱 𝘂𝘀𝗲𝗿 𝗳𝗿𝘂𝘀𝘁𝗿𝗮𝘁𝗶𝗼𝗻, even if it's not perfect.\n📊 Quantify issues: Log incident frequency and business impact. This data will sharpen priorities and guide improvements, 𝗲𝘀𝗽𝗲𝗰𝗶𝗮𝗹𝗹𝘆 𝗳𝗼𝗿 𝘆𝗼𝘂𝗿 𝗴𝘂𝗮𝗿𝗱𝗿𝗮𝗶𝗹, the most critical component of your system.\n3. Regression Testing and Guardrails: The Hidden Heroes Your 𝗴𝘂𝗮𝗿𝗱𝗿𝗮𝗶𝗹 is the 𝗺𝗼𝘀𝘁 𝗰𝗿𝗶𝘁𝗶𝗰𝗮𝗹 𝗽𝗮𝗿𝘁 of your multi-agent system. It's your system's gateway. 𝗬𝗼𝘂𝗿 𝗳𝗶𝗿𝘀𝘁 𝗮𝗻𝗱 𝗹𝗮𝘀𝘁 𝗹𝗶𝗻𝗲 𝗼𝗳 𝗱𝗲𝗳𝗲𝗻𝘀𝗲. 𝗙𝗶𝗿𝘀𝘁 𝗹𝗶𝗻𝗲 (keeps the chaos out) when it's the input guardrail:\nhelps you catch out of scope/unexpected inputs (things your workflows weren't designed to handle). With these kinds of input, you're almost guaranteed to get unreliable, unpredictable results. It's also your security guard: detecting jailbreaks, prompt injections, and malicious inputs. 𝗟𝗮𝘀𝘁 𝗹𝗶𝗻𝗲 (keeps the chaos in) when it's the output guardrail:\nIt helps drastically reduce hallucinations. It can flag out-of-scope topics that slipped past the input guardrail. It should also enforce brand moderation, keeping responses aligned with company image and policies. Think of building a 𝗿𝗲𝗹𝗶𝗮𝗯𝗹𝗲 𝗴𝘂𝗮𝗿𝗱𝗿𝗮𝗶𝗹 as a hard 𝗰𝗹𝗮𝘀𝘀𝗶𝗳𝗶𝗰𝗮𝘁𝗶𝗼𝗻 𝗽𝗿𝗼𝗯𝗹𝗲𝗺. Like any classification task, you're tuning the threshold between false positives (blocking good inputs) and false negatives (letting bad ones through).\n𝗧𝗵𝗲 𝗰𝗵𝗮𝗹𝗹𝗲𝗻𝗴𝗲?\nYour input and output is human language: open-ended, messy, context-dependent. The distribution is huge, and the ambiguity is real. There's no perfect line.\n𝗛𝗲𝗿𝗲 𝗮𝗿𝗲 𝘀𝗼𝗺𝗲 𝘁𝗶𝗽𝘀 𝗶𝗳 𝘆𝗼𝘂'𝗿𝗲 𝘁𝗮𝗰𝗸𝗹𝗶𝗻𝗴 𝘁𝗵𝗶𝘀 𝗽𝗿𝗼𝗯𝗹𝗲𝗺:\n✅ Define clear and concise policies: Set strong guidelines for how your guardrail should behave.\n✅ Inject necessary information: Feed the guardrail only with data needed to make the right decision.\n✅ Define good and bad examples: Help it learn what should be let through - and what shouldn't.\n✅ Test different LLM models: Your guardrail is your most important agent. Pay the necessary price to have best quality.\n✅ Evaluate performance: Use metrics like accuracy, recall, and precision. Better yet, build custom metrics tied to business impact.\n𝗦𝗼𝗹𝗶𝗱 𝗴𝘂𝗮𝗿𝗱𝗿𝗮𝗶𝗹𝘀 𝗮𝗿𝗲 𝘄𝗵𝗮𝘁 𝘀𝗲𝗽𝗮𝗿𝗮𝘁𝗲 𝘃𝗮𝗹𝘂𝗮𝗯𝗹𝗲 𝗔𝗜 𝘀𝘆𝘀𝘁𝗲𝗺𝘀 𝗳𝗿𝗼𝗺 𝗰𝗵𝗮𝗼𝘁𝗶𝗰 𝗼𝗻𝗲𝘀.\n4. Evaluation inside the Agentic Loop Data extraction evaluation matters in agentic systems When building an agentic system, it's important to have data points 𝘁𝗵𝗮𝘁 𝗰𝗮𝗻 𝗯𝗲 𝗰𝗵𝗲𝗰𝗸𝗲𝗱 𝗮𝘁 𝘁𝗵𝗲 𝗵𝗲𝗮𝗿𝘁 𝗼𝗳 𝘆𝗼𝘂𝗿 𝗽𝗿𝗼𝗰𝗲𝘀𝘀 𝘁𝗼 𝗲𝗻𝘀𝘂𝗿𝗲 𝘆𝗼𝘂'𝗿𝗲 𝗼𝗻 𝘁𝗵𝗲 𝗿𝗶𝗴𝗵𝘁 𝗽𝗮𝘁𝗵.\nThink of these as intermediary milestones, deliverables you're asking your multi-agent system to produce on its way to the ultimate objective.\nThese checkpoints serve a dual purpose:\n𝗧𝗮𝗻𝗴𝗶𝗯𝗹𝗲 𝗠𝗲𝗮𝘀𝘂𝗿𝗲𝗺𝗲𝗻𝘁𝘀: They provide clear metrics that simplify system monitoring and facilitate the evaluation of each agent's performance. 𝗣𝗿𝗼𝗰𝗲𝘀𝘀 𝗜𝗻𝘀𝗶𝗴𝗵𝘁𝘀: They offer valuable data that can enrich your understanding of the automated process and even serve as a source of structured knowledge about your users. Moreover, these data points should be used to establish hard controls, dictating actions that agents must or must not take based on the extracted information.\nAdditionally, this metadata functions as a form of memory. Ensuring that agents get this information at critical steps of the process and that it's never lost.\n𝗪𝗮𝗻𝘁 𝗺𝗼𝗿𝗲 𝗿𝗲𝗹𝗶𝗮𝗯𝗹𝗲 𝗮𝗴𝗲𝗻𝘁𝘀?\n𝗦𝘁𝗮𝗿𝘁 𝗱𝗲𝘀𝗶𝗴𝗻𝗶𝗻𝗴 𝘄𝗶𝘁𝗵 𝘁𝗵𝗲𝘀𝗲 𝗶𝗻𝘁𝗲𝗿𝗻𝗮𝗹 𝗰𝗵𝗲𝗰𝗸𝗽𝗼𝗶𝗻𝘁𝘀 𝗶𝗻 𝗺𝗶𝗻𝗱.\n5. Building for Scale and Automation R\u0026amp;D is not optional. Be ready to pay for experimentation. If you want to bring cutting-edge AI systems to production, you'll need to build an R\u0026amp;D mindset into your team and invest in experimentation.\nThe challenge?\nKeep experimentation 𝗳𝗮𝘀𝘁 𝗮𝗻𝗱 𝗮𝗳𝗳𝗼𝗿𝗱𝗮𝗯𝗹𝗲.\nYou want feedback in 𝗱𝗮𝘆𝘀, 𝗻𝗼𝘁 𝘄𝗲𝗲𝗸𝘀.\nAnd just like any expert team, to move fast you need the right tooling.\nAs your production matures and your agent tasks become more complex, your tools need to scale with you.\nAt NORMA, when we started building multi-agent systems, we hit a clear bottleneck: Maintaining constant quality was slowing down our shipping velocity.\nSo, we built something.\n𝗔 𝘁𝗼𝗼𝗹 𝘁𝗼 𝘁𝗲𝘀𝘁 𝗔𝗜 𝗮𝗴𝗲𝗻𝘁𝘀 𝗲𝗻𝗱 𝘁𝗼 𝗲𝗻𝗱, 𝗮𝘁 𝘀𝗰𝗮𝗹𝗲.\nIt started as an internal utility.\nToday, it's a full platform for teams who need to control agent quality over time and evaluate changes across versions efficiently.\nYou want reliable and efficient AI? 𝗠𝗮𝗸𝗲 𝘁𝗲𝘀𝘁𝗶𝗻𝗴 𝗲𝗮𝘀𝘆 𝗳𝗼𝗿 𝘆𝗼𝘂𝗿 𝘁𝗲𝗮𝗺!\n𝗪𝗵𝘆 𝘁𝗲𝘀𝘁𝗶𝗻𝗴 𝗶𝘀 𝗵𝗮𝗿𝗱𝗲𝗿 𝗶𝗻 𝗔𝗜 𝗽𝗹𝗮𝘁𝗳𝗼𝗿𝗺:\nYour input is challenging to control and fully cover. LLMs are everywhere in your pipeline, each bringing potential variance to your process. Many steps rely on structured outputs from LLMs, which aren't always easy to obtain systematically. Agents may access external sources you don't fully control. Outputs vary widely: generated text, finite decision spaces, data extraction... AI frameworks themselves are still evolving quickly and might introduce instabilities or deprecate parts of your code overnight. For all these reasons, developers often struggle to build new features while maintaining high production quality and stability.\n𝗗𝗼𝗻'𝘁 𝗼𝘃𝗲𝗿-𝗽𝗹𝗮𝗻, 𝘀𝘁𝗮𝗿𝘁 𝗮𝘂𝘁𝗼𝗺𝗮𝘁𝗶𝗻𝗴 𝘁𝗲𝘀𝘁𝘀 𝗻𝗼𝘄 𝘁𝗼 𝗿𝗲𝗹𝗶𝗲𝘃𝗲 𝘆𝗼𝘂𝗿 𝗱𝗲𝘃 𝘁𝗲𝗮𝗺.\nAnything that reduces developer workload is worth pursuing. For example:\nBegin with a simple script that automatically runs tests and saves results to an accessible folder. Execute these tests in parallel to accelerate feedback. Provide a clear dashboard or interface for easily inspecting results. Encourage your team to establish metrics to continuously monitor overall quality. Start by measuring outputs globally, then prioritize evaluating the most critical sub-steps, usually those handling data manipulation.\nAfter several iterations, your developers will become comfortable with evaluation processes, naturally integrating these tests to ensure high-quality features. At NORMA, we’ve even automated evaluation directly into our CI/CD pipeline for every PR 🔥\nCheck our quik demo here: ","link":"https://jawad.dev/blog/multiagents_lessons/","section":"blog","tags":"AI,Production,Evaluation","title":"From PoC to Production"},{"body":"","link":"https://jawad.dev/tags/production/","section":"tags","title":"Production"},{"body":"","link":"https://jawad.dev/tags/","section":"tags","title":"Tags"},{"body":"","link":"https://jawad.dev/tags/entrepreneurship/","section":"tags","title":"Entrepreneurship"},{"body":"On May 22, 2025, I had the opportunity and privilege to speak in front of a group of selected young students participating in a hackathon organized by hackai. My mission was simple yet deeply meaningful to me: to openly share my journey as an entrepreneur who started as a developer, highlighting the lessons learned, the challenges I encountered, and the personal growth opportunities I've discovered along the way.\nBeing an introvert myself, I particularly wanted to offer insights and practical advice to those who, like me, dream of achieving greater freedom and self-realization through entrepreneurship, despite (or perhaps even because of) their quieter personalities.\nWhen I started my entrepreneurial journey, I quickly observed two main types of businesses. Some are profitable from day one, usually service-based businesses. Others require initial funding to develop their profitability. Given my French-Moroccan cultural background, bootstrapping, which is building a business with minimal investment, was naturally my prefered choice. Our relationship with risk and money tends to be cautious, often shaped by insecurity rather than optimism (yeah... that's the truth).\nBeyond the financial aspects, entrepreneurship has taught me invaluable lessons about personal growth and ambition. Early on, I understood the importance of fully embracing my strengths and refusing mediocrity. Yet this mindset sometimes caused friction: people might interpret ambition as arrogance or unnecessary perfectionism. I learned not to compromise on my standards just because others saw things differently. Money is indeed essential, but for entrepreneurs, it primarily serves to improve the likelihood of our project's success, not simply as an end goal.\nAn essential realization was that entrepreneurship means solving problems for other people. Genuine curiosity about others' lives and difficulties is foundational. If you're seeking tranquility or a predictable life, entrepreneurship probably isn't the right path. For me, entrepreneurship became a continuous practice of stepping outside my comfort zone, understanding that there's always something new and important to master.\nComfort is tricky: you need a baseline level to sustain long-term efforts, but too much comfort can stifle growth. I found it crucial to regularly push my boundaries, constantly absorbing new skills and knowledge.\nOver time, I also realized entrepreneurship isn't necessarily the ultimate ambition for everyone nor should it be. At one point, I considered joining an existing team as a CTO, thinking I might be happier in a secondary role. The right opportunity never emerged, not because good teams didn't exist, but simply due to timing. Eventually, I found people I'd gladly join, reminding me how critical timing and context are in our decisions.\nAnother powerful lesson was about humility. I observed brilliant individuals from strong academic backgrounds who often fell prey to a superiority complex. Believing success was owed to them, they underestimated the constant hard work required. Entrepreneurship rapidly dispels any illusions of entitlement.\nOne crucial practice I've adopted is always giving my best to people who trust and follow me. Nothing is more powerful than a unified team, and that unity is built on genuine effort and mutual respect. However, this requires trust—sometimes blind trust in others. While some experiences might disappoint you, investing your time and trust in people consistently opens new opportunities.\nI had to become ok with discomfort in other ways too—like paying upfront to learn or taking risks by selling ideas before fully executing them. Initially, this went against my natural instinct for caution, but it proved crucial in creating new opportunities and connections.\n'Two metrics emerged as especially meaningful for me: the number of new people I meet each day and my ability to maintain quality conversations with them.' I've worked consciously to improve these metrics, finding that they directly impacted my entrepreneurial success.\nInstead of dwelling on what I lacked or what seemed impossible, I learned to focus strictly on the value I could deliver and the actions within my control. Playing the victim never advanced anyone's goals. Though we don't all begin our journey from the same point, it's more productive and satisfying to concentrate on possibilities rather than obstacles.\nFailures inevitably happened. 'I also learned not to expect constant support from close friends or family.' Initially, their perceived lack of support frustrated me, but I eventually understood that their apparent reluctance was not personal; often, they were already burdened by their own concerns and fears.\nAccepting my weaknesses was another critical breakthrough. Being average in certain areas won't necessarily limit success. Ego and imposter syndrome plagued me frequently, but gradually diminished as I learned to see them simply as temporary feelings rather than truths.\nTo grow effectively, I discovered that hiring or collaborating with people smarter or more capable than myself was indispensable. I used to hesitate, worried that this could highlight my shortcomings. Now, I seek out these relationships deliberately, knowing they're the quickest path to improvement.\nMy perspective expanded further when I began exploring economics and game theory, understanding that entrepreneurship is about far more than hard skills. Working on my resilience also allowed my creativity to flourish. 'Building resilience will buy you enough time to spark innovation.'\nAlongside resilience, learning to work comfortably in teams and exposing myself regularly to criticism proved immensely beneficial. It strengthened my confidence and clarified my ideas. This also required a conscious effort never to judge prematurely. I've realized repeatedly that good and bad qualities coexist in everyone. There's potential common ground with any person if you look openly enough and if it aligns with your business goals.\nProfessionalism, perhaps more than any other single quality, became fundamental in my entrepreneurial life. Clearly communicating intentions, and consistently doing what you promise, is powerful and irreplaceable. Yes, some people inevitably take advantage of your goodwill, but I've learned that this isn't always negative, provided it helps move your project forward positively.\nAbove all, entrepreneurship demands courage. Not just the courage to lead but also to openly depend on others. Initially, I was hesitant to ask for help, mistakenly seeing self-reliance as strength. Entrepreneurship quickly corrected this misconception, as trying to handle everything alone became exhausting and counterproductive. Learning to ask for help turned out to be one of my most important lessons.\nLastly, I learned not to take myself too seriously. I couldn't control every outcome or constantly sacrifice my own well-being for others. Sacrifices inevitably happen, yet choosing entrepreneurship in a field I genuinely love—m (math and technology) made these sacrifices easier to accept.\nEveryone defines success differently. For entrepreneurs, the relationship between time invested and immediate financial reward can feel frustratingly disconnected. I had to gradually separate the value of my time from immediate compensation. Results often arrive later, sometimes much later, as the business grows. This delayed gratification felt destabilizing at first, especially when accustomed to direct compensation.\nUltimately, my choices affect other people's lives. I've had to accept responsibility for the relationships, opportunities, and even problems my decisions create.\n","link":"https://jawad.dev/blog/5-years-of-norma/","section":"blog","tags":"Entrepreneurship,PersonalGrowth","title":"Lessons learned (so far) as an introvert entrepreneur"},{"body":"","link":"https://jawad.dev/tags/personalgrowth/","section":"tags","title":"PersonalGrowth"},{"body":"","link":"https://jawad.dev/blog/","section":"blog","title":"Blog"},{"body":"","link":"https://jawad.dev/categories/","section":"categories","title":"Categories"},{"body":"\rHi! My name is Jawad. welcome to my personal website I'm Jawad Alaoui, CEO and co-founder of norma.dev, where we develop AI-driven software solutions.\nWith over 20 years in tech, my expertise spans machine learning, data engineering, and product innovation.\nI also serve as a part-time lecturer at Université Gustave Eiffel, sharing insights on AI and data science.\nPassionate about craftsmanship and agility, I advocate for decentralized, remote-first work models.\nAs CTO at indie-plaza.eu, I contribute to helping indie game studios navigate European funding opportunities.\n","link":"https://jawad.dev/","section":"","title":"Jawad ALAOUI"}]